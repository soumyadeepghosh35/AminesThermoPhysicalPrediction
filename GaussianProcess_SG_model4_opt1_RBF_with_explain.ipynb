{"cells":[{"cell_type":"markdown","source":["Write the changes:\n","\n","\n","*   Changed kernel from 'RQ' to 'RBF'  \n","*   Other options need to check: Matern12, Matern32, Matern52"],"metadata":{"id":"ZOahETnQE6dD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBDvv1ovzT0x"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"iM6nU4t9TtdM"},"source":["Install all functions here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1elHSlG5AqV"},"outputs":[],"source":["! pip install rdkit requests gpflow tensorflow_probability scipy scikit-learn scikit-multilearn"]},{"cell_type":"markdown","metadata":{"id":"G1iG44r8TVMW"},"source":["All imports will go here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjFc5DtJTTw4"},"outputs":[],"source":["import pandas as pd\n","import pandas as pd\n","import requests\n","import time\n","from rdkit import Chem\n","import json"]},{"cell_type":"markdown","metadata":{"id":"QZV2C_rg1wWh"},"source":["Load the Raw data"]},{"cell_type":"markdown","metadata":{"id":"vSidHY9Thkbc"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rAkVUbQ11_4"},"outputs":[],"source":["dataDir = '/content/drive/Shareddrives/GCCP/GCCP/Raw_data_files/'\n","outputDir = '/content/drive/Shareddrives/GCCP/GCCP/Output_files/'\n","modelBuildingDataDir = '/content/drive/Shareddrives/GCCP/GCCP/Data_Model_Building/'"]},{"cell_type":"markdown","metadata":{"id":"9z2fJMSVhnwe"},"source":["! mkdir /content/drive/Shareddrives/GCCP/GCCP/Output_files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeqD-T9tylmo"},"outputs":[],"source":["JRGC_Data_dF = pd.read_csv(dataDir + 'JRGC_Data_cleaned.csv')\n","JRGC_Data_dF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POUsysa3A51O"},"outputs":[],"source":["Hvap_CRC_dF = pd.read_csv(modelBuildingDataDir+'Hvap_prediction_data_fcl_with_N.csv')\n","Pc_CRC_dF = pd.read_csv(modelBuildingDataDir+'Pc_prediction_data_fcl_with_N.csv')\n","Vc_CRC_dF = pd.read_csv(modelBuildingDataDir+'Vc_prediction_data_fcl_with_N.csv')\n","Tc_CRC_dF = pd.read_csv(modelBuildingDataDir+'Tc_prediction_data_fcl_with_N.csv')\n","Tb_CRC_dF = pd.read_csv(modelBuildingDataDir+'Tb_prediction_data_fcl_with_N.csv')\n","Tm_CRC_dF = pd.read_csv(modelBuildingDataDir+'Tm_prediction_data_fcl_with_N.csv')\n","logP_CRC_dF = pd.read_csv(modelBuildingDataDir+'logP_prediction_data_fcl_with_N.csv')"]},{"cell_type":"markdown","metadata":{"id":"Yh1TFgAecvnx"},"source":["Train Gaussian Processes (GP) on physicochemical properties"]},{"cell_type":"markdown","metadata":{"id":"STESUTDt16Bv"},"source":["Combining all properties in a single code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFzqbZ2xjkby"},"outputs":[],"source":["method_number = 4"]},{"cell_type":"markdown","source":["explanation of codes"],"metadata":{"id":"FoMkA1x3whKR"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Script to train a GP on physicochemical properties.\n","\"\"\"\n","\n","\n","import os\n","import warnings\n","import time\n","\n","# Specific\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from skmultilearn.model_selection import iterative_train_test_split\n","import gpflow\n","from gpflow.utilities import print_summary, set_trainable, deepcopy\n","import tensorflow as tf\n","from tensorflow_probability import bijectors as tfb\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# =============================================================================\n","# Auxiliary Functions\n","# =============================================================================\n","\n","\n","def set_white_exp_95CI(code):\n","    \"\"\"\n","    Sets the estimated average ~95% confidence interval on labels using the property code\n","\n","    Parameters:\n","    code : string\n","        Property code\n","\n","    Returns:\n","    exp_95CI : float\n","    \"\"\"\n","    if code == 'Tb':\n","        exp_95CI = 10.0\n","    elif code == 'Tm':\n","        exp_95CI = 5.0\n","    elif code == 'Hvap':\n","        exp_95CI = 1.0\n","\n","\n","def gpConfig_from_method(method_number, code, kernel = 'RBF', anisotropic = False, useWhiteKernel = True, trainLikelihood = True, opt_method = 'L-BFGS-B'):\n","    \"\"\"\n","    Creates a gpConfig dictionary based on the method number.\n","\n","    Parameters:\n","    method_number : int\n","        Method number.\n","\n","    Returns:\n","    gpConfig : dictionary\n","        Dictionary of GP configuration parameters.\n","\n","    Note:\n","    method_number is used to define which type of gp model to use\n","    1: Y = GP(0, K(Mw, Y_gc))\n","    2: Y - Y_gc = GP(0, K(Mw))\n","    3: Y - Y_gc = GP(0, K(Mw, Y_gc))\n","    4: Y = GP(Y_gc, K(Mw, Y_gc))\n","    5: Y = GP(AMw + BY_gc + c, K(Mw, Y_gc))\n","    \"\"\"\n","    gpConfig={'kernel': kernel,\n","           'useWhiteKernel':useWhiteKernel,\n","           'trainLikelihood':trainLikelihood,\n","           'opt_method':opt_method,\n","           'anisotropic':anisotropic}\n","    if method_number == 1:\n","        gpConfig['mean_function']='Zero'\n","        gpConfig['Name']='y_exp = GP(0, K(x1,x2))'\n","        gpConfig['SaveName']='model_1'\n","    if method_number == 2:\n","        gpConfig['mean_function']='Zero'\n","        gpConfig['Name']='y_exp = y_GC + GP(0, K(x1))'\n","        gpConfig['SaveName']='model_2'\n","    if method_number == 3:\n","        gpConfig['mean_function']='Constant'\n","        gpConfig['Name']='y_exp = GP(y_GC, K(x1,x2))'\n","        gpConfig['SaveName']='model_3'\n","    if method_number == 4:\n","        gpConfig['mean_function']='Linear'\n","        gpConfig['Name']='y_exp = GP(B@X, K(x1,x2))'\n","        gpConfig['SaveName']='model_4'\n","    else:\n","        if method_number not in [1 , 2, 3, 4]:\n","            raise ValueError('invalid method number input')\n","    return gpConfig\n","\n","\n","def get_gp_data(X, Y, method_number):\n","    \"\"\"\n","    Gets X and Y data to train GP based on the method number\n","\n","    Parameters:\n","    X : numpy array\n","        Features data.\n","    Y : numpy array\n","        Property data.\n","    method_number : int\n","        Method number\n","\n","    Returns:\n","    X_gp : numpy array\n","        Features data to train GP.\n","    Y_gp: numpy array\n","        Data to train GP.\n","    Y_gc: numpy array\n","        Data from Joback method\n","    \"\"\"\n","    if method_number == 2:\n","        X_gp = X[:,0].reshape(-1,1)\n","    else:\n","        X_gp = X\n","    if method_number == 2:\n","        Y_gp = Y - X[:,1]\n","    else:\n","        Y_gp = Y\n","    Y_gp = Y_gp.reshape(-1,1)\n","    Y_gc = X[:,1].reshape(-1,1)\n","    return X_gp, Y_gp, Y_gc\n","\n","\n","def discrepancy_to_property(method_number, y_pred, y_gc, idx):\n","    \"\"\"\n","    Adds discrepancy to property based on the method number\n","    Parameters:\n","    method_number : int\n","        Method number\n","    y_pred : numpy array\n","        GP predicted output\n","    y_gc : numpy array\n","        Predicted GC method results\n","    idx : np.array\n","        Index of the y_gc to be added to y_pred\n","\n","    Returns:\n","    y_prop : numpy array\n","        Predicted property value\n","    \"\"\"\n","    if method_number == 2:\n","        y_prop = y_pred + y_gc[idx.flatten(),:]\n","    else:\n","        y_prop = y_pred\n","    return y_prop\n","\n","def stratifyvector(Y):\n","    \"\"\"\n","    Creates a stratified vector based on the label data Y\n","\n","    Parameters:\n","    Y : numpy array\n","        label data\n","    Returns:\n","    stratifyVector : numpy array\n","        Stratified vector\n","    \"\"\"\n","    # Iterate over number of bins, trying to find the larger number of bins that\n","    # guarantees at least 5 values per bin\n","    for n in range(1,100):\n","        # Bin Y using n bins\n","        stratifyVector=pd.cut(Y,n,labels=False)\n","        # Define isValid (all bins have at least 5 values)\n","        isValid=True\n","        # Check that all bins have at least 5 values\n","        for k in range(n):\n","            if np.count_nonzero(stratifyVector==k)<5:\n","                isValid=False\n","        #If isValid is false, n is too large; nBins must be the previous iteration\n","        if not isValid:\n","            nBins=n-1\n","            break\n","    # Generate vector for stratified splitting based on labels\n","    stratifyVector=pd.cut(Y,nBins,labels=False)\n","    return stratifyVector\n","\n","def normalize(inputArray,skScaler=None,method='Standardization',reverse=False):\n","    \"\"\"\n","    normalize() normalizes (or unnormalizes) inputArray using the method\n","    specified and the skScaler provided.\n","\n","    Parameters\n","    ----------\n","    inputArray : numpy array\n","        Array to be normalized. If dim>1, array is normalized column-wise.\n","    skScaler : scikit-learn preprocessing object or None\n","        Scikit-learn preprocessing object previosly fitted to data. If None,\n","        the object is fitted to inputArray.\n","        Default: None\n","    method : string, optional\n","        Normalization method to be used.\n","        Methods available:\n","            . Standardization - classic standardization, (x-mean(x))/std(x)\n","            . MinMax - scale to range (0,1)\n","            . LogStand - standardization on the log of the variable,\n","                         (log(x)-mean(log(x)))/std(log(x))\n","            . Log+bStand - standardization on the log of variables that can be\n","                           zero; uses a small buffer,\n","                           (log(x+b)-mean(log(x+b)))/std(log(x+b))\n","        Default: 'Standardization'\n","    reverse : bool\n","        Whether  to normalize (False) or unnormalize (True) inputArray.\n","        Defalt: False\n","\n","    Returns\n","    -------\n","    inputArray : numpy array\n","        Normalized (or unnormalized) version of inputArray.\n","    skScaler : scikit-learn preprocessing object\n","        Scikit-learn preprocessing object fitted to inputArray. It is the same\n","        as the inputted skScaler, if it was provided.\n","\n","    \"\"\"\n","    # If inputArray is a labels vector of size (N,), reshape to (N,1)\n","    if inputArray.ndim==1:\n","        inputArray=inputArray.reshape((-1,1))\n","        warnings.warn('Input to normalize() was of shape (N,). It was assumed'\\\n","                      +' to be a column array and converted to a (N,1) shape.')\n","    # If skScaler is None, train for the first time\n","    if method == 'None':\n","        inputArray=inputArray\n","        skScaler = None\n","    else:\n","        if skScaler is None:\n","            # Check method\n","            if method=='Standardization' or method=='MinMax': aux=inputArray\n","            elif method=='LogStand': aux=np.log(inputArray)\n","            elif method=='Log+bStand': aux=np.log(inputArray+10**-3)\n","            else: raise ValueError('Could not recognize method in normalize().')\n","            if method!='MinMax':\n","                skScaler=preprocessing.StandardScaler().fit(aux)\n","            else:\n","                skScaler=preprocessing.MinMaxScaler().fit(aux)\n","        # Do main operation (normalize or unnormalize)\n","        if reverse:\n","            # Rescale the data back to its original distribution\n","            inputArray=skScaler.inverse_transform(inputArray)\n","            # Check method\n","            if method=='LogStand': inputArray=np.exp(inputArray)\n","            elif method=='Log+bStand': inputArray=np.exp(inputArray)-10**-3\n","        elif not reverse:\n","            # Check method\n","            if method=='Standardization' or method=='MinMax': aux=inputArray\n","            elif method=='LogStand': aux=np.log(inputArray)\n","            elif method=='Log+bStand': aux=np.log(inputArray+10**-3)\n","            else: raise ValueError('Could not recognize method in normalize().')\n","            inputArray=skScaler.transform(aux)\n","    # Return\n","    return inputArray,skScaler\n","\n","\n","\n","# Build GPR model function with bounded hyperparameters\n","def build_model_with_bounded_params(X, Y, kern, low, high, \\\n","                                    high_alpha, init_val1, init_val2, init_val3, \\\n","                                    useWhite, trainLikelihood, anisotropic, typeMeanFunc):\n","    \"\"\"\n","    build_model_with_bounded_params(*) creates a GP model object with bounded hyperparameters and initial\n","    values\n","\n","    Parameters\n","    ----------\n","    X : numpy array\n","        Feature data\n","    Y : numpy array\n","        Label data\n","    low : float\n","        lower bound on all hyperparameters\n","    high : float\n","        upper bound on all hyperparameters except alpha for the RQ kernel\n","    high_alpha : float\n","        upper bound on alpha hyperparameter for the RQ kernel\n","    init_val1 : float\n","        initial values for first length scale and alpha parameter\n","    init_val2 : float\n","        initial values for second length scale for anisotropic kernels\n","        for isotropic kernels, only one initial value (init_val1) is used\n","    init_val1 : float\n","        initial values for variance or scale hyperparameter of kernel 1 (not Whitenoise kernel)\n","\n","    Returns\n","    -------\n","    model : Gpflow model object\n","        GP model object with bounded hyperparameters and initial values\n","\n","    \"\"\"\n","\n","    low = tf.cast(low, dtype=tf.float64)\n","    high = tf.cast(high, dtype=tf.float64)\n","    high_alpha = tf.cast(high_alpha, dtype=tf.float64)\n","    init_val1 = tf.cast(init_val1, dtype=tf.float64)\n","    init_val2 = tf.cast(init_val2, dtype=tf.float64)\n","    init_val3 = tf.cast(init_val3, dtype=tf.float64)\n","    if anisotropic == True:\n","        lsc = gpflow.Parameter([init_val1, init_val2], transform=tfb.Sigmoid(low , high), dtype=tf.float64)\n","    else:\n","        lsc = gpflow.Parameter(init_val1, transform=tfb.Sigmoid(low , high), dtype=tf.float64)\n","    alf = gpflow.Parameter(init_val1, transform=tfb.Sigmoid(low , high_alpha), dtype=tf.float64)\n","    var = gpflow.Parameter(init_val3, transform=tfb.Sigmoid(low , high), dtype=tf.float64)\n","    if kern == \"RQ\":\n","        kernel_ = gpflow.kernels.RationalQuadratic()\n","        kernel_.alpha = alf\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    elif kern == \"RBF\":\n","        kernel_ = gpflow.kernels.RBF()\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    elif kern == \"Matern12\":\n","        kernel_ = gpflow.kernels.Matern12()\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    elif kern == \"Matern32\":\n","        kernel_ = gpflow.kernels.Matern32()\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    elif kern == \"Matern52\":\n","        kernel_ = gpflow.kernels.Matern52()\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    if useWhite == True:\n","        #white_var = np.array(np.random.uniform(0.05, 1.0))\n","        final_kernel = kernel_+gpflow.kernels.White(variance=1.0)\n","    else:\n","        final_kernel = kernel_\n","\n","    if typeMeanFunc == 'Zero':\n","        mf = None\n","    if typeMeanFunc == 'Constant':\n","        #If constant value is selected but no value is given, default to zero mean\n","        mf_val = np.array([0,1]).reshape(-1,1)\n","        mf = gpflow.functions.Linear(mf_val)\n","    if typeMeanFunc == 'Linear':\n","        A = np.ones((X.shape[1],1))\n","        mf = gpflow.functions.Linear(A)\n","    model_ = gpflow.models.GPR(data=(X, Y), kernel=final_kernel, mean_function=mf, noise_variance=10**-5)\n","    if typeMeanFunc == 'Constant':\n","        gpflow.set_trainable(model_.mean_function.A, False)\n","        gpflow.set_trainable(model_.mean_function.b, False)\n","    gpflow.utilities.set_trainable(model_.likelihood.variance,trainLikelihood)\n","    model = model_\n","    return model\n","\n","\n","\n","def buildGP(X_Train, Y_Train, gpConfig, code, featurenorm, retrain_count):\n","    \"\"\"\n","    buildGP() builds and fits a GP model using the training data provided.\n","\n","    Parameters\n","    ----------\n","    X_Train : numpy array (N,K)\n","        Training features, where N is the number of data points and K is the\n","        number of independent features\n","    Y_Train : numpy array (N,1)\n","        Training labels (e.g., property of a given molecule).\n","    gpConfig : dictionary, optional\n","        Dictionary containing the configuration of the GP. If a key is not\n","        present in the dictionary, its default value is used.\n","        Keys:\n","            . kernel : string\n","                Kernel to be used. One of:\n","                    . 'RBF' - gpflow.kernels.RBF()\n","                    . 'RQ' - gpflow.kernels.RationalQuadratic()\n","                    . 'Matern12' - gpflow.kernels.Matern12()\n","                    . 'Matern32' - gpflow.kernels.Matern32()\n","                    . 'Matern52' - gpflow.kernels.Matern52()\n","                The default is 'RQ'.\n","            . useWhiteKernel : boolean\n","                Whether to use a White kernel (gpflow.kernels.White).\n","                The default is True.\n","            . trainLikelihood : boolean\n","                Whether to treat the variance of the likelihood of the modeal\n","                as a trainable (or fitting) parameter. If False, this value is\n","                fixed at 10^-5.\n","                The default is True.\n","        The default is {}.\n","    sc_y_scale : Scikit learn standard scaler object\n","        standard scaler fitted on label training data\n","    retrain_count : int\n","        Current GP retrain number\n","\n","    \"\"\"\n","    # Unpack gpConfig\n","    kernel=gpConfig.get('kernel','RQ')\n","    useWhiteKernel=gpConfig.get('useWhiteKernel','True')\n","    trainLikelihood=gpConfig.get('trainLikelihood','True')\n","    typeMeanFunc=gpConfig.get('mean_function','Zero')\n","    opt_method=gpConfig.get('opt_method','L-BFGS-B')\n","    anisotropy=gpConfig.get('anisotropic','False')\n","\n","    seed_ = int(retrain_count) * 100\n","    np.random.seed(seed_)\n","    tf.random.set_seed(seed_)\n","\n","    if retrain_count == 0:\n","        init_val1 = 1\n","        init_val2 = 1\n","        init_val3 = 1\n","    else:\n","        init_val1 = np.array(np.random.uniform(0, 100))\n","        init_val2 = np.array(np.random.uniform(0, 100))\n","        init_val3 = np.array(np.random.lognormal(0, 1.0))\n","\n","    model = build_model_with_bounded_params(X_Train, Y_Train, kernel, 0.00001, 100, 5000, init_val1, \\\n","                            init_val2, init_val3, useWhiteKernel, trainLikelihood, anisotropy, typeMeanFunc)\n","    model_pretrain = deepcopy(model)\n","    # print(gpflow.utilities.print_summary(model))\n","    condition_number = np.linalg.cond(model.kernel(X_Train))\n","    # Build optimizer\n","    optimizer=gpflow.optimizers.Scipy()\n","    # Fit GP to training data\n","    aux=optimizer.minimize(model.training_loss,\n","                           model.trainable_variables,\n","                           options={'maxiter':10**9},\n","                           method=opt_method)\n","    obj_func = model.training_loss()\n","    if aux.success:\n","        opt_success = True\n","    else:\n","        opt_success = False\n","\n","    return model, aux, condition_number, obj_func, opt_success, retrain_count, model_pretrain\n","\n","\n","\n","def train_gp(X_Train, Y_Train, gpConfig, code, sc_y, featurenorm, retrain_GP, retrain_count):\n","    \"\"\"\n","    Trains the GP given training data.\n","\n","\n","    \"\"\"\n","\n","    # Train the model multiple times and keep track of the model with the lowest minimum training loss\n","    best_minimum_loss = float('inf')\n","    best_model = None\n","    best_model_pretrain = None\n","    best_model_success = False\n","    best_condition_num = float('inf')\n","    args = (X_Train, Y_Train, gpConfig)\n","\n","    retrain_GP = int(retrain_GP)\n","    retrain_count = retrain_count\n","    for i in range(retrain_GP):\n","        model, aux, condition_number, obj_func, opt_success, retrain_count, model_pretrain = \\\n","            buildGP(X_Train, Y_Train, gpConfig, code, featurenorm, retrain_count)\n","        print(f\"training_loss = {obj_func}\")\n","        print(f\"condition_number = {condition_number}\")\n","        retrain_count += 1\n","        if best_minimum_loss > obj_func and opt_success==True:\n","            best_minimum_loss = obj_func\n","            best_model = model\n","            best_model_pretrain = model_pretrain\n","            best_model_success = opt_success\n","            best_condition_num = condition_number\n","    if best_model_success == False:\n","        warnings.warn('GP optimizer failed to converge with retrains')\n","\n","    #Put hyperparameters in a list\n","    trained_hyperparams = gpflow.utilities.read_values(best_model)\n","\n","    if sc_y != None:\n","        sc_y_scale = sc_y.scale_\n","    else:\n","        sc_y_scale = None\n","\n","    return best_model,best_minimum_loss,best_model_success,best_condition_num,trained_hyperparams,best_model_pretrain,sc_y_scale\n","\n","\n","def gpPredict(model,X):\n","    \"\"\"\n","    gpPredict() returns the prediction and variance of the GP model\n","    on the X data provided.\n","\n","    Parameters\n","    ----------\n","    model : gpflow.models.gpr.GPR object\n","        GP model.\n","    X : numpy array (N,K)\n","        Training features, where N is the number of data points and K is the\n","        number of independent features (e.g., sigma profile bins).\n","\n","    Returns\n","    -------\n","    Y : numpy array (N,1)\n","        GP predictions.\n","    STD : numpy array (N,1)\n","        GP standard deviations.\n","\n","    \"\"\"\n","    # Do GP prediction, obtaining mean and variance\n","    GP_Mean,GP_Var=model.predict_f(X)\n","    # Convert to numpy\n","    GP_Mean=GP_Mean.numpy()\n","    GP_Var=GP_Var.numpy()\n","    # Prepare outputs\n","    Y=GP_Mean\n","    VAR=GP_Var\n","    # Output\n","    return Y,VAR\n","\n","\n","\n","\n","def count_outside_95(Y_Train, Y_Test, Y_Train_Pred, Y_Test_Pred, Y_Train_CI, Y_Test_CI):\n","    \"\"\"\n","    count_outside_95() finds the number and fraction of predicted data that are outside the predicted 95%\n","        confidence intervals from the true values\n","\n","    Parameters:\n","    Y_Train_CI : numpy array\n","        Absolute values of the 95% confidence interval on the predictions on training set\n","    Y_Test_CI : numpy array\n","        Absolute values of the 95% confidence interval on the predictions on testing set\n","\n","    \"\"\"\n","    out_95_train = []\n","    out_95_test = []\n","    for index, value in enumerate(Y_Train):\n","        if np.abs(value - Y_Train_Pred[index]) > Y_Train_CI[index]:\n","            out_95_train.append(index)\n","    num_out95_train = len(out_95_train)\n","    frac_out95_train = num_out95_train/len(Y_Train)\n","    for index, value in enumerate(Y_Test):\n","        if np.abs(value - Y_Test_Pred[index]) > Y_Test_CI[index]:\n","            out_95_test.append(index)\n","    num_out95_test = len(out_95_test)\n","    frac_out95_test = num_out95_test/len(Y_Test)\n","\n","    return num_out95_train, frac_out95_train, num_out95_test, frac_out95_test\n","\n","\n","# =============================================================================\n","# Statistical Metrics and Plotting Functions\n","# =============================================================================\n","def calculate_metrics(y_true, y_pred):\n","    \"\"\"Calculate comprehensive statistical metrics\"\"\"\n","    y_true = np.array(y_true).flatten()\n","    y_pred = np.array(y_pred).flatten()\n","\n","    mae = mean_absolute_error(y_true, y_pred)\n","    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n","    r2 = r2_score(y_true, y_pred)\n","\n","    # MAPE and MPE with zero-division protection\n","    mask = y_true != 0\n","    if mask.sum() > 0:\n","        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n","        mpe = np.mean((y_true[mask] - y_pred[mask]) / y_true[mask]) * 100\n","    else:\n","        mape = np.inf\n","        mpe = np.inf\n","\n","    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape, 'MPE': mpe}\n","\n","def calculate_improvement(train_metrics, test_metrics):\n","    \"\"\"Calculate improvement metrics (positive values indicate overfitting)\"\"\"\n","    improvement = {}\n","\n","    # For R2: higher is better, so improvement = train - test\n","    improvement['R2'] = train_metrics['R2'] - test_metrics['R2']\n","\n","    # For error metrics: lower is better, so improvement = test - train\n","    improvement['MAE'] = test_metrics['MAE'] - train_metrics['MAE']\n","    improvement['RMSE'] = test_metrics['RMSE'] - train_metrics['RMSE']\n","    improvement['MAPE'] = test_metrics['MAPE'] - train_metrics['MAPE']\n","    improvement['MPE'] = test_metrics['MPE'] - train_metrics['MPE']\n","\n","    return improvement\n","\n","def plot_predictions(Y_train_true, Y_train_pred, Y_test_true, Y_test_pred,\n","                    Y_train_ci, Y_test_ci, property_name, save_path):\n","    \"\"\"Create side-by-side parity plots: Train, Test, and Combined\"\"\"\n","\n","    # Set style\n","    plt.style.use('default')\n","    sns.set_palette(\"husl\")\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n","    fig.suptitle(f'GP Predictions for {property_name}', fontsize=16, fontweight='bold')\n","\n","    # Flatten arrays\n","    y_train_true_flat = Y_train_true.flatten()\n","    y_train_pred_flat = Y_train_pred.flatten()\n","    y_train_ci_flat = Y_train_ci.flatten()\n","    y_test_true_flat = Y_test_true.flatten()\n","    y_test_pred_flat = Y_test_pred.flatten()\n","    y_test_ci_flat = Y_test_ci.flatten()\n","\n","    # Calculate metrics\n","    train_metrics = calculate_metrics(y_train_true_flat, y_train_pred_flat)\n","    test_metrics = calculate_metrics(y_test_true_flat, y_test_pred_flat)\n","\n","    # Combined data for the third plot\n","    y_combined_true = np.concatenate([y_train_true_flat, y_test_true_flat])\n","    y_combined_pred = np.concatenate([y_train_pred_flat, y_test_pred_flat])\n","    y_combined_ci = np.concatenate([y_train_ci_flat, y_test_ci_flat])\n","    combined_metrics = calculate_metrics(y_combined_true, y_combined_pred)\n","\n","    # Plot 1: Training Set\n","    ax1 = axes[0]\n","    # Changed: True on x-axis, Predicted on y-axis\n","    ax1.scatter(y_train_true_flat, y_train_pred_flat, alpha=0.6, s=30, color='blue')\n","    ax1.errorbar(y_train_true_flat, y_train_pred_flat, yerr=y_train_ci_flat,\n","                fmt='none', alpha=0.3, color='blue')\n","\n","    min_val = min(y_train_true_flat.min(), y_train_pred_flat.min())\n","    max_val = max(y_train_true_flat.max(), y_train_pred_flat.max())\n","    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, linewidth=2)\n","\n","    ax1.set_title(f'Training Set (n={len(y_train_true_flat)})\\n'\n","                  f'R² = {train_metrics[\"R2\"]:.3f}, RMSE = {train_metrics[\"RMSE\"]:.2f}\\n'\n","                  f'MAE = {train_metrics[\"MAE\"]:.2f}, MAPE = {train_metrics[\"MAPE\"]:.1f}%')\n","    ax1.set_xlabel(f'True {property_name}')        # Changed: True on x-axis\n","    ax1.set_ylabel(f'Predicted {property_name}')   # Changed: Predicted on y-axis\n","    ax1.grid(True, alpha=0.3)\n","\n","    # Plot 2: Test Set\n","    ax2 = axes[1]\n","    # Changed: True on x-axis, Predicted on y-axis\n","    ax2.scatter(y_test_true_flat, y_test_pred_flat, alpha=0.6, s=30, color='red')\n","    ax2.errorbar(y_test_true_flat, y_test_pred_flat, yerr=y_test_ci_flat,\n","                fmt='none', alpha=0.3, color='red')\n","\n","    min_val = min(y_test_true_flat.min(), y_test_pred_flat.min())\n","    max_val = max(y_test_true_flat.max(), y_test_pred_flat.max())\n","    ax2.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, linewidth=2)\n","\n","    ax2.set_title(f'Test Set (n={len(y_test_true_flat)})\\n'\n","                  f'R² = {test_metrics[\"R2\"]:.3f}, RMSE = {test_metrics[\"RMSE\"]:.2f}\\n'\n","                  f'MAE = {test_metrics[\"MAE\"]:.2f}, MAPE = {test_metrics[\"MAPE\"]:.1f}%')\n","    ax2.set_xlabel(f'True {property_name}')        # Changed: True on x-axis\n","    ax2.set_ylabel(f'Predicted {property_name}')   # Changed: Predicted on y-axis\n","    ax2.grid(True, alpha=0.3)\n","\n","    # Plot 3: Combined Train + Test Set\n","    ax3 = axes[2]\n","    # Changed: True on x-axis, Predicted on y-axis\n","    ax3.scatter(y_train_true_flat, y_train_pred_flat, alpha=0.5, s=25, color='blue', label='Training')\n","    ax3.scatter(y_test_true_flat, y_test_pred_flat, alpha=0.5, s=25, color='red', label='Test')\n","    ax3.errorbar(y_combined_true, y_combined_pred, yerr=y_combined_ci,\n","                fmt='none', alpha=0.2, color='gray')\n","\n","    min_val = min(y_combined_true.min(), y_combined_pred.min())\n","    max_val = max(y_combined_true.max(), y_combined_pred.max())\n","    ax3.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, linewidth=2)\n","\n","    ax3.set_title(f'Combined (n={len(y_combined_true)})\\n'\n","                  f'R² = {combined_metrics[\"R2\"]:.3f}, RMSE = {combined_metrics[\"RMSE\"]:.2f}\\n'\n","                  f'MAE = {combined_metrics[\"MAE\"]:.2f}, MAPE = {combined_metrics[\"MAPE\"]:.1f}%')\n","    ax3.set_xlabel(f'True {property_name}')        # Changed: True on x-axis\n","    ax3.set_ylabel(f'Predicted {property_name}')   # Changed: Predicted on y-axis\n","    ax3.grid(True, alpha=0.3)\n","    ax3.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"{save_path}/{property_name}_GP_results.png\", dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","    return train_metrics, test_metrics, combined_metrics\n","\n","\n","def process_single_property(code, kernel, anisotropic, opt_method, useWhiteKernel,\n","                          trainLikelihood, retrain_GP, method_number, outputDir,\n","                          modelBuildingDataDir, seed=42):\n","    \"\"\"Process a single property code\"\"\"\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"PROCESSING PROPERTY: {code}\")\n","    print(f\"{'='*80}\")\n","\n","    # Set random seed for reproducibility\n","    np.random.seed(seed)\n","\n","    # Define normalization methods\n","    if method_number == 2:\n","        featureNorm, labelNorm = 'None', 'None'\n","    else:\n","        featureNorm, labelNorm = 'Standardization', 'Standardization'\n","\n","    # GP Configuration\n","    gpConfig = gpConfig_from_method(method_number, code, kernel, anisotropic, useWhiteKernel, trainLikelihood, opt_method)\n","\n","    try:\n","        # Load data\n","        data_file = f\"{modelBuildingDataDir}{code}_prediction_data_fcl_with_N.csv\"\n","        print(f\"Loading data for {code} from {data_file}\")\n","\n","        if os.path.exists(data_file):\n","            df = pd.read_csv(data_file)\n","            print(f\"Shape of {data_file}: {df.shape}\")\n","        else:\n","            print(f\"File not found: {data_file}\")\n","\n","        db = pd.read_csv(data_file)\n","        # Only drop rows where feature columns (2:-1) or target column (-1) have NaN\n","        feature_target_cols = list(db.columns[2:])  # Features + target columns\n","        db = db.dropna(subset=feature_target_cols)\n","        X = db.iloc[:,2:-1].copy().to_numpy('float')\n","        data_names = db.columns.tolist()[2:]\n","        Y = db.iloc[:,-1].copy().to_numpy('float')\n","        Y = Y.reshape(-1,1)\n","        Y_gc = X[:,-1].reshape(-1,1)\n","        MW = X[:,-2].reshape(-1,1)\n","\n","        print(f\"Loaded data for {code}: {X.shape[0]} samples, {X.shape[1]} features\")\n","\n","        # Stratification and train-test split\n","        X_data = db.iloc[:,2:-1].copy()\n","        num_rows_X = X_data.shape[0]\n","        y_data_dum = (np.ones((num_rows_X, 2))).astype(int)\n","        indices = np.arange(X_data.shape[0])\n","        y_stratify = np.column_stack((indices, y_data_dum))\n","        X_stratify = X_data.values\n","\n","        X_ = np.array(y_stratify)\n","        y_ = np.array(X_stratify)\n","        y_strat = y_\n","        X_strat = X_\n","\n","        np.random.seed(seed)\n","        X_Train_0, y_Train_0, X_valTest_0, y_valTest_0 = iterative_train_test_split(X_strat, y_strat, test_size = 0.2)\n","\n","        train_indices = (X_Train_0[:,0]).astype(int)\n","        test_indices = (X_valTest_0[:,0]).astype(int)\n","\n","        trn_idx = train_indices\n","        test_idx = test_indices\n","\n","        X_Train_0 = X[trn_idx, :]\n","        X_Test_0 = X[test_idx, :]\n","        Y_Train_0 = Y[trn_idx, :]\n","        Y_Test_0 = Y[test_idx, :]\n","\n","        X_Train, Y_Train, Y_gc_Train = get_gp_data(X_Train_0, Y_Train_0[:,-1], method_number)\n","        X_Test, Y_Test, Y_gc_Test = get_gp_data(X_Test_0, Y_Test_0[:,-1], method_number)\n","\n","        train_data = np.concatenate((X_Train, Y_Train), axis = 1)\n","        test_data = np.concatenate((X_Test, Y_Test), axis = 1)\n","\n","        if method_number == 2:\n","            data_names =  data_names[:1] + [data_names[-1] + \" Discrepancy\"]\n","\n","        train_df = pd.DataFrame(train_data, columns = data_names)\n","        test_df = pd.DataFrame(test_data, columns = data_names)\n","\n","        # Create output directory\n","        property_outputDir = f\"{outputDir}{code}/{gpConfig['SaveName']}\"\n","        print(f\"Output directory: {property_outputDir}\")\n","        os.makedirs(property_outputDir, exist_ok = True)\n","        train_df.to_csv(f\"{property_outputDir}/train_data.csv\", index= False)\n","        test_df.to_csv(f\"{property_outputDir}/test_data.csv\", index= False)\n","\n","        # Normalize\n","        X_Train_N = X_Train.copy()\n","        X_Test_N = X_Test.copy()\n","        Y_Train_N = Y_Train.copy()\n","        Y_gc_Train_N = Y_gc_Train.copy()\n","\n","        if featureNorm is not None:\n","            X_Train_N, skScaler_X = normalize(X_Train, method=featureNorm)\n","            X_Test_N, __ = normalize(X_Test, method=featureNorm, skScaler=skScaler_X)\n","        else:\n","            skScaler_X = None\n","\n","        if labelNorm is not None:\n","            Y_Train_N, skScaler_Y = normalize(Y_Train, method=labelNorm)\n","            Y_gc_Train_N, __ = normalize(Y_gc_Train, method=labelNorm, skScaler=skScaler_Y)\n","        else:\n","            skScaler_Y = None\n","\n","        # Train GP\n","        args = (X_Train_N, Y_Train_N, gpConfig)\n","        retrain_count = 0\n","        model, best_min_loss, fit_success, cond_num, trained_hyperparams, model_pretrain, sc_y_scale = \\\n","            train_gp(X_Train_N, Y_Train_N, gpConfig, code, skScaler_Y, featureNorm, retrain_GP, retrain_count)\n","\n","        best_lml = -1 * best_min_loss\n","        best_lml = best_lml.numpy()\n","        print(f\"Best LML: {best_lml}, Fit Success: {fit_success}, Condition Number: {cond_num}\")\n","\n","        # Save model summary\n","        model_file_name = f\"{property_outputDir}/model_summary.txt\"\n","        with open(model_file_name, 'w') as file:\n","            val = gpflow.utilities.read_values(model)\n","            file.write(str(val))\n","            file.write(f\"\\n Condition Number: {cond_num}\")\n","            file.write(f\"\\n Fit Success?: {fit_success}\")\n","            file.write(f\"\\n Log-marginal Likelihood: {best_lml}\")\n","\n","        # Get GP predictions\n","        Y_Train_Pred_N, Y_Train_Var_N = gpPredict(model, X_Train_N)\n","        Y_Test_Pred_N, Y_Test_Var_N = gpPredict(model, X_Test_N)\n","\n","        # Unnormalize\n","        Y_Train_Pred = Y_Train_Pred_N.copy()\n","        Y_Test_Pred = Y_Test_Pred_N.copy()\n","        Y_Train_Var = Y_Train_Var_N.copy()\n","        Y_Test_Var = Y_Test_Var_N.copy()\n","\n","        if labelNorm != 'None':\n","            Y_Train_Pred, __ = normalize(Y_Train_Pred_N, skScaler=skScaler_Y,\n","                                        method=labelNorm, reverse=True)\n","            Y_Test_Pred, __ = normalize(Y_Test_Pred_N, skScaler=skScaler_Y,\n","                                       method=labelNorm, reverse=True)\n","            Y_Train_Var = (skScaler_Y.scale_**2) * Y_Train_Var\n","            Y_Test_Var = (skScaler_Y.scale_**2) * Y_Test_Var\n","\n","        # Get data in form such that Y train and Y test are the actual property predictions\n","        if method_number == 2:\n","            Y_Test_Pred_plt = Y_Test_Pred + Y_gc_Test\n","            Y_Train_Pred_plt = Y_Train_Pred + Y_gc_Train\n","            Y_Test_plt = Y_Test + Y_gc_Test\n","            Y_Train_plt = Y_Train + Y_gc_Train\n","        else:\n","            Y_Test_Pred_plt = Y_Test_Pred\n","            Y_Train_Pred_plt = Y_Train_Pred\n","            Y_Test_plt = Y_Test\n","            Y_Train_plt = Y_Train\n","\n","        Y_Test_CI_plt = 1.96 * np.sqrt(Y_Test_Var)\n","        Y_Train_CI_plt = 1.96 * np.sqrt(Y_Train_Var)\n","\n","        count_CI = count_outside_95(Y_Train_plt, Y_Test_plt,\n","                         Y_Train_Pred_plt, Y_Test_Pred_plt,\n","                         Y_Train_CI_plt, Y_Test_CI_plt)\n","        count_CI = np.array(count_CI)\n","\n","        # Save numerical results\n","        np.savetxt(f\"{property_outputDir}/{code}_count_CI.txt\", count_CI)\n","        np.savetxt(f\"{property_outputDir}/{code}_train_indices.txt\", trn_idx)\n","        np.savetxt(f\"{property_outputDir}/{code}_test_indices.txt\", test_idx)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_train_true.txt\", Y_Train_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_test_true.txt\", Y_Test_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_train_pred.txt\", Y_Train_Pred_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_test_pred.txt\", Y_Test_Pred_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_gc_train.txt\", Y_gc_Train)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_gc_test.txt\", Y_gc_Test)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_train_pred_95CI.txt\", Y_Train_CI_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_test_pred_95CI.txt\", Y_Test_CI_plt)\n","\n","        # Calculate and print statistical metrics\n","        print(f\"\\nSTATISTICAL METRICS FOR {code}\")\n","        print(\"-\" * 50)\n","\n","        train_metrics = calculate_metrics(Y_Train_plt, Y_Train_Pred_plt)\n","        test_metrics = calculate_metrics(Y_Test_plt, Y_Test_Pred_plt)\n","        improvement_metrics = calculate_improvement(train_metrics, test_metrics)\n","\n","        # Print metrics table\n","        print(f\"{'Metric':<10} {'Training':<12} {'Test':<12} {'Improvement':<12}\")\n","        print(\"-\" * 50)\n","        for metric in ['MAE', 'RMSE', 'R2', 'MAPE', 'MPE']:\n","            train_val = train_metrics[metric]\n","            test_val = test_metrics[metric]\n","            improvement_val = improvement_metrics[metric]\n","\n","            if metric in ['MAE', 'RMSE']:\n","                print(f\"{metric:<10} {train_val:<12.4f} {test_val:<12.4f} {improvement_val:<12.4f}\")\n","            elif metric == 'R2':\n","                print(f\"{metric:<10} {train_val:<12.4f} {test_val:<12.4f} {improvement_val:<12.4f}\")\n","            else:  # MAPE, MPE (percentages)\n","                print(f\"{metric:<10} {train_val:<12.2f}% {test_val:<12.2f}% {improvement_val:<12.2f}%\")\n","\n","        # Save metrics to CSV\n","        metrics_df = pd.DataFrame({\n","            'Metric': list(train_metrics.keys()),\n","            'Training': list(train_metrics.values()),\n","            'Test': list(test_metrics.values()),\n","            'Improvement': list(improvement_metrics.values())\n","        })\n","        metrics_df.to_csv(f\"{property_outputDir}/{code}_metrics.csv\", index=False)\n","\n","        # Create and save plots\n","        print(f\"Generating plots for {code}...\")\n","        train_metrics_plot, test_metrics_plot, combined_metrics_plot = plot_predictions(\n","            Y_Train_plt, Y_Train_Pred_plt, Y_Test_plt, Y_Test_Pred_plt,\n","            Y_Train_CI_plt, Y_Test_CI_plt, code, property_outputDir\n","        )\n","\n","        # Print confidence interval analysis\n","        print(f\"\\nConfidence Interval Analysis for {code}:\")\n","        print(f\"Training: {count_CI[0]}/{len(Y_Train_plt)} ({count_CI[1]*100:.1f}%) outside 95% CI\")\n","        print(f\"Test: {count_CI[2]}/{len(Y_Test_plt)} ({count_CI[3]*100:.1f}%) outside 95% CI\")\n","\n","        # Create summary dictionary\n","        results_summary = {\n","            'Property': code,\n","            'Method': method_number,\n","            'Kernel': kernel,\n","            'N_Train': len(Y_Train_plt),\n","            'N_Test': len(Y_Test_plt),\n","            'N_Total': len(Y_Train_plt) + len(Y_Test_plt),\n","            'LML': best_lml,\n","            'Condition_Number': cond_num,\n","            'Fit_Success': fit_success,\n","            'Train_R2': train_metrics['R2'],\n","            'Test_R2': test_metrics['R2'],\n","            'Combined_R2': combined_metrics_plot['R2'],\n","            'Train_MAE': train_metrics['MAE'],\n","            'Test_MAE': test_metrics['MAE'],\n","            'Combined_MAE': combined_metrics_plot['MAE'],\n","            'Train_RMSE': train_metrics['RMSE'],\n","            'Test_RMSE': test_metrics['RMSE'],\n","            'Combined_RMSE': combined_metrics_plot['RMSE'],\n","            'Train_MAPE': train_metrics['MAPE'],\n","            'Test_MAPE': test_metrics['MAPE'],\n","            'Combined_MAPE': combined_metrics_plot['MAPE'],\n","            'Train_MPE': train_metrics['MPE'],\n","            'Test_MPE': test_metrics['MPE'],\n","            'Combined_MPE': combined_metrics_plot['MPE'],\n","            'R2_Improvement': improvement_metrics['R2'],\n","            'MAE_Improvement': improvement_metrics['MAE'],\n","            'RMSE_Improvement': improvement_metrics['RMSE'],\n","            'MAPE_Improvement': improvement_metrics['MAPE'],\n","            'MPE_Improvement': improvement_metrics['MPE'],\n","            'CI_Train_Outside': count_CI[1],\n","            'CI_Test_Outside': count_CI[3]\n","        }\n","\n","        # Save summary to JSON\n","        import json\n","        with open(f\"{property_outputDir}/{code}_summary.json\", 'w') as f:\n","            json.dump(results_summary, f, indent=4, default=str)\n","\n","        print(f\"\\nResults saved to: {property_outputDir}\")\n","        print(f\"Files created:\")\n","        print(f\"  - {code}_GP_results.png (side-by-side plots)\")\n","        print(f\"  - {code}_metrics.csv (metrics table with improvement)\")\n","        print(f\"  - {code}_summary.json (complete summary)\")\n","\n","        return results_summary\n","\n","    except Exception as e:\n","        print(f\"Error processing {code}: {str(e)}\")\n","        import traceback\n","        traceback.print_exc()\n","        return None\n","\n","def print_comprehensive_metrics_table(all_results):\n","    \"\"\"Print a comprehensive table of statistical metrics for all properties\"\"\"\n","\n","    if not all_results:\n","        print(\"No results to display.\")\n","        return\n","\n","    print(f\"\\n{'='*120}\")\n","    print(f\"COMPREHENSIVE STATISTICAL METRICS TABLE - ALL PROPERTIES\")\n","    print(f\"{'='*120}\")\n","\n","    # Define metrics to display\n","    metrics = ['R2', 'MAE', 'RMSE', 'MAPE', 'MPE']\n","\n","    # Create header\n","    header = f\"{'Property':<10}\"\n","    for dataset in ['Train', 'Test', 'Combined']:\n","        header += f\"{'':>20}{dataset:<20}{'':>20}\"\n","    print(header)\n","\n","    # Create sub-header with metric names\n","    subheader = f\"{'':>10}\"\n","    for dataset in ['Train', 'Test', 'Combined']:\n","        subheader += f\"{'R²':<8}{'MAE':<8}{'RMSE':<8}{'MAPE':<8}{'MPE':<8}{'':>5}\"\n","    print(subheader)\n","\n","    print(\"-\" * 120)\n","\n","    # Print data for each property\n","    for result in all_results:\n","        property_name = result['Property']\n","        line = f\"{property_name:<10}\"\n","\n","        # Train metrics\n","        line += f\"{result['Train_R2']:<8.3f}\"\n","        line += f\"{result['Train_MAE']:<8.2f}\"\n","        line += f\"{result['Train_RMSE']:<8.2f}\"\n","        line += f\"{result['Train_MAPE']:<8.1f}\"\n","        line += f\"{result['Train_MPE']:<8.1f}\"\n","        line += f\"{'':>5}\"\n","\n","        # Test metrics\n","        line += f\"{result['Test_R2']:<8.3f}\"\n","        line += f\"{result['Test_MAE']:<8.2f}\"\n","        line += f\"{result['Test_RMSE']:<8.2f}\"\n","        line += f\"{result['Test_MAPE']:<8.1f}\"\n","        line += f\"{result['Test_MPE']:<8.1f}\"\n","        line += f\"{'':>5}\"\n","\n","        # Combined metrics\n","        line += f\"{result['Combined_R2']:<8.3f}\"\n","        line += f\"{result['Combined_MAE']:<8.2f}\"\n","        line += f\"{result['Combined_RMSE']:<8.2f}\"\n","        line += f\"{result['Combined_MAPE']:<8.1f}\"\n","        line += f\"{result['Combined_MPE']:<8.1f}\"\n","\n","        print(line)\n","\n","    print(\"-\" * 120)\n","\n","    # Calculate and print summary statistics\n","    print(f\"\\nSUMMARY STATISTICS ACROSS ALL PROPERTIES:\")\n","    print(f\"{'Metric':<15}{'Train':<15}{'Test':<15}{'Combined':<15}\")\n","    print(\"-\" * 60)\n","\n","    for metric in ['R2', 'MAE', 'RMSE', 'MAPE', 'MPE']:\n","        train_key = f'Train_{metric}'\n","        test_key = f'Test_{metric}'\n","        combined_key = f'Combined_{metric}'\n","\n","        train_values = [r[train_key] for r in all_results if not np.isnan(r[train_key]) and not np.isinf(r[train_key])]\n","        test_values = [r[test_key] for r in all_results if not np.isnan(r[test_key]) and not np.isinf(r[test_key])]\n","        combined_values = [r[combined_key] for r in all_results if not np.isnan(r[combined_key]) and not np.isinf(r[combined_key])]\n","\n","        if train_values and test_values and combined_values:\n","            train_mean = np.mean(train_values)\n","            test_mean = np.mean(test_values)\n","            combined_mean = np.mean(combined_values)\n","\n","            print(f\"{metric + ' (avg)':<15}{train_mean:<15.3f}{test_mean:<15.3f}{combined_mean:<15.3f}\")\n","\n","\n","def print_performance_ranking_table(all_results):\n","    \"\"\"Print properties ranked by performance\"\"\"\n","\n","    if not all_results:\n","        return\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"PROPERTY PERFORMANCE RANKING (by Test R²)\")\n","    print(f\"{'='*80}\")\n","\n","    # Sort by Test R2 (descending)\n","    sorted_results = sorted(all_results, key=lambda x: x['Test_R2'], reverse=True)\n","\n","    print(f\"{'Rank':<6}{'Property':<10}{'Test R²':<10}{'Test RMSE':<12}{'Test MAE':<10}{'Overfitting':<12}\")\n","    print(\"-\" * 80)\n","\n","    for i, result in enumerate(sorted_results, 1):\n","        # Calculate overfitting indicator (Train R2 - Test R2)\n","        overfitting = result['Train_R2'] - result['Test_R2']\n","        overfitting_indicator = \"High\" if overfitting > 0.1 else \"Moderate\" if overfitting > 0.05 else \"Low\"\n","\n","        print(f\"{i:<6}{result['Property']:<10}{result['Test_R2']:<10.3f}\"\n","              f\"{result['Test_RMSE']:<12.2f}{result['Test_MAE']:<10.2f}{overfitting_indicator:<12}\")\n","\n","    print(\"-\" * 80)\n","\n","    # Identify best and worst\n","    best = sorted_results[0]\n","    worst = sorted_results[-1]\n","\n","    print(f\"\\nBest performing property: {best['Property']} (R² = {best['Test_R2']:.3f})\")\n","    print(f\"Worst performing property: {worst['Property']} (R² = {worst['Test_R2']:.3f})\")\n","    print(f\"Performance range: {worst['Test_R2']:.3f} to {best['Test_R2']:.3f}\")\n","\n","\n","def save_comprehensive_metrics_to_csv(all_results, output_dir):\n","    \"\"\"Save detailed metrics table to CSV\"\"\"\n","\n","    if not all_results:\n","        return\n","\n","    # Create detailed metrics DataFrame\n","    detailed_data = []\n","\n","    for result in all_results:\n","        # Create one row per property with all metrics\n","        row = {\n","            'Property': result['Property'],\n","            'Method': result['Method'],\n","            'Kernel': result['Kernel'],\n","            'N_Train': result['N_Train'],\n","            'N_Test': result['N_Test'],\n","            'N_Total': result['N_Total'],\n","\n","            # Training metrics\n","            'Train_R2': result['Train_R2'],\n","            'Train_MAE': result['Train_MAE'],\n","            'Train_RMSE': result['Train_RMSE'],\n","            'Train_MAPE': result['Train_MAPE'],\n","            'Train_MPE': result['Train_MPE'],\n","\n","            # Test metrics\n","            'Test_R2': result['Test_R2'],\n","            'Test_MAE': result['Test_MAE'],\n","            'Test_RMSE': result['Test_RMSE'],\n","            'Test_MAPE': result['Test_MAPE'],\n","            'Test_MPE': result['Test_MPE'],\n","\n","            # Combined metrics\n","            'Combined_R2': result['Combined_R2'],\n","            'Combined_MAE': result['Combined_MAE'],\n","            'Combined_RMSE': result['Combined_RMSE'],\n","            'Combined_MAPE': result['Combined_MAPE'],\n","            'Combined_MPE': result['Combined_MPE'],\n","\n","            # Improvement metrics\n","            'R2_Improvement': result['R2_Improvement'],\n","            'MAE_Improvement': result['MAE_Improvement'],\n","            'RMSE_Improvement': result['RMSE_Improvement'],\n","            'MAPE_Improvement': result['MAPE_Improvement'],\n","            'MPE_Improvement': result['MPE_Improvement'],\n","\n","            # Other metrics\n","            'LML': result['LML'],\n","            'Condition_Number': result['Condition_Number'],\n","            'Fit_Success': result['Fit_Success'],\n","            'CI_Train_Outside': result['CI_Train_Outside'],\n","            'CI_Test_Outside': result['CI_Test_Outside']\n","        }\n","        detailed_data.append(row)\n","\n","    # Create DataFrame and save\n","    detailed_df = pd.DataFrame(detailed_data)\n","    detailed_df.to_csv(f\"{output_dir}comprehensive_metrics_table.csv\", index=False)\n","\n","    # Create a simplified summary table\n","    summary_data = []\n","    for result in all_results:\n","        summary_row = {\n","            'Property': result['Property'],\n","            'Train_R2': result['Train_R2'],\n","            'Train_RMSE': result['Train_RMSE'],\n","            'Test_R2': result['Test_R2'],\n","            'Test_RMSE': result['Test_RMSE'],\n","            'Combined_R2': result['Combined_R2'],\n","            'Combined_RMSE': result['Combined_RMSE'],\n","            'Overfitting_R2': result['R2_Improvement'],\n","            'N_Train': result['N_Train'],\n","            'N_Test': result['N_Test']\n","        }\n","        summary_data.append(summary_row)\n","\n","    summary_df = pd.DataFrame(summary_data)\n","    summary_df.to_csv(f\"{output_dir}summary_metrics_table.csv\", index=False)\n","\n","    print(f\"\\nDetailed metrics saved to: {output_dir}comprehensive_metrics_table.csv\")\n","    print(f\"Summary metrics saved to: {output_dir}summary_metrics_table.csv\")"],"metadata":{"id":"mP143OX7Wh_f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tb, Tm, H_vap"],"metadata":{"id":"nxTal-5qL1u-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMXFIxknkou-"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    # =============================================================================\n","    # Configuration - Modify these paths as needed\n","    # =============================================================================\n","\n","    # Set your data directory path here\n","    dataDir = '/content/drive/Shareddrives/GCCP/GCCP/Raw_data_files/'\n","    outputDir = '/content/drive/Shareddrives/GCCP/GCCP/Output_files/'\n","    modelBuildingDataDir = '/content/drive/Shareddrives/GCCP/GCCP/Data_Model_Building/'\n","\n","    # Property codes to process\n","    property_codes = ['Tb', 'Tm', 'Hvap']\n","    #property_codes = ['Tc']\n","\n","    # GP Configuration parameters\n","    kernel = 'RBF'  # Options: RQ, RBF, Matern12, Matern32, Matern52\n","    anisotropic = False\n","    opt_method = 'L-BFGS-B'  # Options: L-BFGS-B, BFGS\n","    useWhiteKernel = True\n","    trainLikelihood = False\n","    retrain_GP = 10\n","    seed = 42\n","\n","    # Initialize timer and results storage\n","    total_start_time = time.time()\n","    all_results = []\n","    successful_properties = []\n","    failed_properties = []\n","\n","    print(f\"{'='*100}\")\n","    print(f\"STARTING GP TRAINING FOR ALL PROPERTIES\")\n","    print(f\"{'='*100}\")\n","    print(f\"Properties to process: {property_codes}\")\n","    print(f\"Method: {method_number}, Kernel: {kernel}, Retrains: {retrain_GP}\")\n","    print(f\"Output directory: {outputDir}\")\n","    print(f\"{'='*100}\")\n","\n","    # Process each property\n","    for i, code in enumerate(property_codes):\n","        property_start_time = time.time()\n","\n","        print(f\"\\n{'#'*60}\")\n","        print(f\"PROCESSING {i+1}/{len(property_codes)}: {code}\")\n","        print(f\"{'#'*60}\")\n","\n","        # Process the property\n","        result = process_single_property(\n","            code=code,\n","            kernel=kernel,\n","            anisotropic=anisotropic,\n","            opt_method=opt_method,\n","            useWhiteKernel=useWhiteKernel,\n","            trainLikelihood=trainLikelihood,\n","            retrain_GP=retrain_GP,\n","            method_number=method_number,\n","            outputDir=outputDir,\n","            modelBuildingDataDir=modelBuildingDataDir,\n","            seed=seed\n","        )\n","\n","        property_end_time = time.time()\n","        property_elapsed = property_end_time - property_start_time\n","\n","        if result is not None:\n","            all_results.append(result)\n","            successful_properties.append(code)\n","            print(f\"\\n✓ {code} completed successfully in {property_elapsed:.2f} seconds\")\n","        else:\n","            failed_properties.append(code)\n","            print(f\"\\n✗ {code} failed after {property_elapsed:.2f} seconds\")\n","\n","    # Create comprehensive summary\n","    total_end_time = time.time()\n","    total_elapsed = total_end_time - total_start_time\n","\n","    print(f\"\\n{'='*100}\")\n","    print(f\"COMPLETE\")\n","    print(f\"{'='*100}\")\n","    print(f\"Total time elapsed: {total_elapsed:.2f} seconds ({total_elapsed/60:.1f} minutes)\")\n","    print(f\"Successfully processed: {len(successful_properties)}/{len(property_codes)} properties\")\n","    print(f\"Successful: {successful_properties}\")\n","    if failed_properties:\n","        print(f\"Failed: {failed_properties}\")\n","\n","    # Save individual results summary (existing code)\n","    if all_results:\n","        # Create overall summary DataFrame\n","        summary_df = pd.DataFrame(all_results)\n","        summary_df.to_csv(f\"{outputDir}all_properties_summary.csv\", index=False)\n","\n","        # NEW: Print comprehensive metrics table\n","        print_comprehensive_metrics_table(all_results)\n","\n","        # NEW: Print performance ranking\n","        print_performance_ranking_table(all_results)\n","\n","        # NEW: Save detailed metrics to CSV\n","        save_comprehensive_metrics_to_csv(all_results, outputDir)\n","\n","        # Quick summary statistics (existing code)\n","        print(f\"\\n{'='*60}\")\n","        print(f\"QUICK SUMMARY\")\n","        print(f\"{'='*60}\")\n","\n","        print(f\"{'Property':<8} {'Test R²':<8} {'Test RMSE':<10} {'Test MAE':<8} {'Train/Test R²':<12}\")\n","        print(\"-\" * 60)\n","        for result in all_results:\n","            r2_ratio = result['Train_R2'] / result['Test_R2'] if result['Test_R2'] > 0 else np.inf\n","            print(f\"{result['Property']:<8} {result['Test_R2']:<8.3f} {result['Test_RMSE']:<10.2f} \"\n","                  f\"{result['Test_MAE']:<8.2f} {r2_ratio:<12.2f}\")\n","\n","        # Best and worst performing properties\n","        best_r2 = max(all_results, key=lambda x: x['Test_R2'])\n","        worst_r2 = min(all_results, key=lambda x: x['Test_R2'])\n","\n","        print(f\"\\nBest R² performance: {best_r2['Property']} (R² = {best_r2['Test_R2']:.3f})\")\n","        print(f\"Worst R² performance: {worst_r2['Property']} (R² = {worst_r2['Test_R2']:.3f})\")\n","\n","        print(f\"\\nAll results saved to: {outputDir}all_properties_summary.csv\")\n","\n","    print(f\"\\n{'='*100}\")\n","    print(\" FINISHED\")\n","    print(f\"{'='*100}\")"]},{"cell_type":"markdown","source":["# H_vap"],"metadata":{"id":"dxuoOadhxQUQ"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # =============================================================================\n","    # Configuration - Modify these paths as needed\n","    # =============================================================================\n","\n","    # Set your data directory path here\n","    dataDir = '/content/drive/Shareddrives/GCCP/GCCP/Raw_data_files/'\n","    outputDir = '/content/drive/Shareddrives/GCCP/GCCP/Output_files/'\n","    modelBuildingDataDir = '/content/drive/Shareddrives/GCCP/GCCP/Data_Model_Building/'\n","\n","    # Property codes to process\n","    property_codes = ['Hvap']\n","    #property_codes = ['Tc']\n","\n","    # GP Configuration parameters\n","    kernel = 'RBF'  # Options: RQ, RBF, Matern12, Matern32, Matern52\n","    anisotropic = False\n","    opt_method = 'L-BFGS-B'  # Options: L-BFGS-B, BFGS\n","    useWhiteKernel = True\n","    trainLikelihood = False\n","    retrain_GP = 10\n","    seed = 42\n","\n","    # Initialize timer and results storage\n","    total_start_time = time.time()\n","    all_results = []\n","    successful_properties = []\n","    failed_properties = []\n","\n","    print(f\"{'='*100}\")\n","    print(f\"STARTING AUTOMATED GP TRAINING FOR ALL PROPERTIES\")\n","    print(f\"{'='*100}\")\n","    print(f\"Properties to process: {property_codes}\")\n","    print(f\"Method: {method_number}, Kernel: {kernel}, Retrains: {retrain_GP}\")\n","    print(f\"Output directory: {outputDir}\")\n","    print(f\"{'='*100}\")\n","\n","    # Process each property\n","    for i, code in enumerate(property_codes):\n","        property_start_time = time.time()\n","\n","        print(f\"\\n{'#'*60}\")\n","        print(f\"PROCESSING {i+1}/{len(property_codes)}: {code}\")\n","        print(f\"{'#'*60}\")\n","\n","        # Process the property\n","        result = process_single_property(\n","            code=code,\n","            kernel=kernel,\n","            anisotropic=anisotropic,\n","            opt_method=opt_method,\n","            useWhiteKernel=useWhiteKernel,\n","            trainLikelihood=trainLikelihood,\n","            retrain_GP=retrain_GP,\n","            method_number=method_number,\n","            outputDir=outputDir,\n","            modelBuildingDataDir=modelBuildingDataDir,\n","            seed=seed\n","        )\n","\n","        property_end_time = time.time()\n","        property_elapsed = property_end_time - property_start_time\n","\n","        if result is not None:\n","            all_results.append(result)\n","            successful_properties.append(code)\n","            print(f\"\\n✓ {code} completed successfully in {property_elapsed:.2f} seconds\")\n","        else:\n","            failed_properties.append(code)\n","            print(f\"\\n✗ {code} failed after {property_elapsed:.2f} seconds\")\n","\n","    # Create comprehensive summary\n","    total_end_time = time.time()\n","    total_elapsed = total_end_time - total_start_time\n","\n","    print(f\"\\n{'='*100}\")\n","    print(f\" COMPLETE\")\n","    print(f\"{'='*100}\")\n","    print(f\"Total time elapsed: {total_elapsed:.2f} seconds ({total_elapsed/60:.1f} minutes)\")\n","    print(f\"Successfully processed: {len(successful_properties)}/{len(property_codes)} properties\")\n","    print(f\"Successful: {successful_properties}\")\n","    if failed_properties:\n","        print(f\"Failed: {failed_properties}\")\n","\n","    # Save individual results summary (existing code)\n","    if all_results:\n","        # Create overall summary DataFrame\n","        summary_df = pd.DataFrame(all_results)\n","        summary_df.to_csv(f\"{outputDir}all_properties_summary.csv\", index=False)\n","\n","        # NEW: Print comprehensive metrics table\n","        print_comprehensive_metrics_table(all_results)\n","\n","        # NEW: Print performance ranking\n","        print_performance_ranking_table(all_results)\n","\n","        # NEW: Save detailed metrics to CSV\n","        save_comprehensive_metrics_to_csv(all_results, outputDir)\n","\n","        # Quick summary statistics (existing code)\n","        print(f\"\\n{'='*60}\")\n","        print(f\"QUICK SUMMARY\")\n","        print(f\"{'='*60}\")\n","\n","        print(f\"{'Property':<8} {'Test R²':<8} {'Test RMSE':<10} {'Test MAE':<8} {'Train/Test R²':<12}\")\n","        print(\"-\" * 60)\n","        for result in all_results:\n","            r2_ratio = result['Train_R2'] / result['Test_R2'] if result['Test_R2'] > 0 else np.inf\n","            print(f\"{result['Property']:<8} {result['Test_R2']:<8.3f} {result['Test_RMSE']:<10.2f} \"\n","                  f\"{result['Test_MAE']:<8.2f} {r2_ratio:<12.2f}\")\n","\n","        # Best and worst performing properties\n","        best_r2 = max(all_results, key=lambda x: x['Test_R2'])\n","        worst_r2 = min(all_results, key=lambda x: x['Test_R2'])\n","\n","        print(f\"\\nBest R² performance: {best_r2['Property']} (R² = {best_r2['Test_R2']:.3f})\")\n","        print(f\"Worst R² performance: {worst_r2['Property']} (R² = {worst_r2['Test_R2']:.3f})\")\n","\n","        print(f\"\\nAll results saved to: {outputDir}all_properties_summary.csv\")\n","\n","    print(f\"\\n{'='*100}\")\n","    print(\" FINISHED\")\n","    print(f\"{'='*100}\")"],"metadata":{"id":"kn6dQ1sJxSaU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pc, Vc, Tc"],"metadata":{"id":"7aBpKbNuLyDM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HoIpaUpFmNfK"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    # =============================================================================\n","    # Configuration - Modify these paths as needed\n","    # =============================================================================\n","\n","    # Set your data directory path here\n","    dataDir = '/content/drive/Shareddrives/GCCP/GCCP/Raw_data_files/'\n","    outputDir = '/content/drive/Shareddrives/GCCP/GCCP/Output_files/'\n","    modelBuildingDataDir = '/content/drive/Shareddrives/GCCP/GCCP/Data_Model_Building/'\n","\n","    # Property codes to process\n","    property_codes = ['Pc', 'Vc', 'Tc']\n","    #property_codes = ['Tc']\n","\n","    # GP Configuration parameters\n","    kernel = 'RBF'  # Options: RQ, RBF, Matern12, Matern32, Matern52\n","    anisotropic = False\n","    opt_method = 'L-BFGS-B'  # Options: L-BFGS-B, BFGS\n","    useWhiteKernel = True\n","    trainLikelihood = False\n","    retrain_GP = 10\n","    seed = 42\n","\n","    # Initialize timer and results storage\n","    total_start_time = time.time()\n","    all_results = []\n","    successful_properties = []\n","    failed_properties = []\n","\n","    print(f\"{'='*100}\")\n","    print(f\"STARTING AUTOMATED GP TRAINING FOR ALL PROPERTIES\")\n","    print(f\"{'='*100}\")\n","    print(f\"Properties to process: {property_codes}\")\n","    print(f\"Method: {method_number}, Kernel: {kernel}, Retrains: {retrain_GP}\")\n","    print(f\"Output directory: {outputDir}\")\n","    print(f\"{'='*100}\")\n","\n","    # Process each property\n","    for i, code in enumerate(property_codes):\n","        property_start_time = time.time()\n","\n","        print(f\"\\n{'#'*60}\")\n","        print(f\"PROCESSING {i+1}/{len(property_codes)}: {code}\")\n","        print(f\"{'#'*60}\")\n","\n","        # Process the property\n","        result = process_single_property(\n","            code=code,\n","            kernel=kernel,\n","            anisotropic=anisotropic,\n","            opt_method=opt_method,\n","            useWhiteKernel=useWhiteKernel,\n","            trainLikelihood=trainLikelihood,\n","            retrain_GP=retrain_GP,\n","            method_number=method_number,\n","            outputDir=outputDir,\n","            modelBuildingDataDir=modelBuildingDataDir,\n","            seed=seed\n","        )\n","\n","        property_end_time = time.time()\n","        property_elapsed = property_end_time - property_start_time\n","\n","        if result is not None:\n","            all_results.append(result)\n","            successful_properties.append(code)\n","            print(f\"\\n✓ {code} completed successfully in {property_elapsed:.2f} seconds\")\n","        else:\n","            failed_properties.append(code)\n","            print(f\"\\n✗ {code} failed after {property_elapsed:.2f} seconds\")\n","\n","    # Create comprehensive summary\n","    total_end_time = time.time()\n","    total_elapsed = total_end_time - total_start_time\n","\n","    print(f\"\\n{'='*100}\")\n","    print(f\" COMPLETE\")\n","    print(f\"{'='*100}\")\n","    print(f\"Total time elapsed: {total_elapsed:.2f} seconds ({total_elapsed/60:.1f} minutes)\")\n","    print(f\"Successfully processed: {len(successful_properties)}/{len(property_codes)} properties\")\n","    print(f\"Successful: {successful_properties}\")\n","    if failed_properties:\n","        print(f\"Failed: {failed_properties}\")\n","\n","    # Save individual results summary (existing code)\n","    if all_results:\n","        # Create overall summary DataFrame\n","        summary_df = pd.DataFrame(all_results)\n","        summary_df.to_csv(f\"{outputDir}all_properties_summary.csv\", index=False)\n","\n","        # NEW: Print comprehensive metrics table\n","        print_comprehensive_metrics_table(all_results)\n","\n","        # NEW: Print performance ranking\n","        print_performance_ranking_table(all_results)\n","\n","        # NEW: Save detailed metrics to CSV\n","        save_comprehensive_metrics_to_csv(all_results, outputDir)\n","\n","        # Quick summary statistics (existing code)\n","        print(f\"\\n{'='*60}\")\n","        print(f\"QUICK SUMMARY\")\n","        print(f\"{'='*60}\")\n","\n","        print(f\"{'Property':<8} {'Test R²':<8} {'Test RMSE':<10} {'Test MAE':<8} {'Train/Test R²':<12}\")\n","        print(\"-\" * 60)\n","        for result in all_results:\n","            r2_ratio = result['Train_R2'] / result['Test_R2'] if result['Test_R2'] > 0 else np.inf\n","            print(f\"{result['Property']:<8} {result['Test_R2']:<8.3f} {result['Test_RMSE']:<10.2f} \"\n","                  f\"{result['Test_MAE']:<8.2f} {r2_ratio:<12.2f}\")\n","\n","        # Best and worst performing properties\n","        best_r2 = max(all_results, key=lambda x: x['Test_R2'])\n","        worst_r2 = min(all_results, key=lambda x: x['Test_R2'])\n","\n","        print(f\"\\nBest R² performance: {best_r2['Property']} (R² = {best_r2['Test_R2']:.3f})\")\n","        print(f\"Worst R² performance: {worst_r2['Property']} (R² = {worst_r2['Test_R2']:.3f})\")\n","\n","        print(f\"\\nAll results saved to: {outputDir}all_properties_summary.csv\")\n","\n","    print(f\"\\n{'='*100}\")\n","    print(\" FINISHED\")\n","    print(f\"{'='*100}\")"]},{"cell_type":"markdown","source":["# logP"],"metadata":{"id":"Yfk6RTg9LucD"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # =============================================================================\n","    # Configuration - Modify these paths as needed\n","    # =============================================================================\n","\n","    # Set your data directory path here\n","    dataDir = '/content/drive/Shareddrives/GCCP/GCCP/Raw_data_files/'\n","    outputDir = '/content/drive/Shareddrives/GCCP/GCCP/Output_files/'\n","    modelBuildingDataDir = '/content/drive/Shareddrives/GCCP/GCCP/Data_Model_Building/'\n","\n","    # Property codes to process\n","    property_codes = ['logP']\n","    #property_codes = ['Tc']\n","\n","    # GP Configuration parameters\n","    kernel = 'RBF'  # Options: RQ, RBF, Matern12, Matern32, Matern52\n","    anisotropic = False\n","    opt_method = 'L-BFGS-B'  # Options: L-BFGS-B, BFGS\n","    useWhiteKernel = True\n","    trainLikelihood = False\n","    retrain_GP = 10\n","    seed = 42\n","\n","    # Initialize timer and results storage\n","    total_start_time = time.time()\n","    all_results = []\n","    successful_properties = []\n","    failed_properties = []\n","\n","    print(f\"{'='*100}\")\n","    print(f\"STARTING AUTOMATED GP TRAINING FOR ALL PROPERTIES\")\n","    print(f\"{'='*100}\")\n","    print(f\"Properties to process: {property_codes}\")\n","    print(f\"Method: {method_number}, Kernel: {kernel}, Retrains: {retrain_GP}\")\n","    print(f\"Output directory: {outputDir}\")\n","    print(f\"{'='*100}\")\n","\n","    # Process each property\n","    for i, code in enumerate(property_codes):\n","        property_start_time = time.time()\n","\n","        print(f\"\\n{'#'*60}\")\n","        print(f\"PROCESSING {i+1}/{len(property_codes)}: {code}\")\n","        print(f\"{'#'*60}\")\n","\n","        # Process the property\n","        result = process_single_property(\n","            code=code,\n","            kernel=kernel,\n","            anisotropic=anisotropic,\n","            opt_method=opt_method,\n","            useWhiteKernel=useWhiteKernel,\n","            trainLikelihood=trainLikelihood,\n","            retrain_GP=retrain_GP,\n","            method_number=method_number,\n","            outputDir=outputDir,\n","            modelBuildingDataDir=modelBuildingDataDir,\n","            seed=seed\n","        )\n","\n","        property_end_time = time.time()\n","        property_elapsed = property_end_time - property_start_time\n","\n","        if result is not None:\n","            all_results.append(result)\n","            successful_properties.append(code)\n","            print(f\"\\n✓ {code} completed successfully in {property_elapsed:.2f} seconds\")\n","        else:\n","            failed_properties.append(code)\n","            print(f\"\\n✗ {code} failed after {property_elapsed:.2f} seconds\")\n","\n","    # Create comprehensive summary\n","    total_end_time = time.time()\n","    total_elapsed = total_end_time - total_start_time\n","\n","    print(f\"\\n{'='*100}\")\n","    print(f\" COMPLETE\")\n","    print(f\"{'='*100}\")\n","    print(f\"Total time elapsed: {total_elapsed:.2f} seconds ({total_elapsed/60:.1f} minutes)\")\n","    print(f\"Successfully processed: {len(successful_properties)}/{len(property_codes)} properties\")\n","    print(f\"Successful: {successful_properties}\")\n","    if failed_properties:\n","        print(f\"Failed: {failed_properties}\")\n","\n","    # Save individual results summary (existing code)\n","    if all_results:\n","        # Create overall summary DataFrame\n","        summary_df = pd.DataFrame(all_results)\n","        summary_df.to_csv(f\"{outputDir}all_properties_summary.csv\", index=False)\n","\n","        # NEW: Print comprehensive metrics table\n","        print_comprehensive_metrics_table(all_results)\n","\n","        # NEW: Print performance ranking\n","        print_performance_ranking_table(all_results)\n","\n","        # NEW: Save detailed metrics to CSV\n","        save_comprehensive_metrics_to_csv(all_results, outputDir)\n","\n","        # Quick summary statistics (existing code)\n","        print(f\"\\n{'='*60}\")\n","        print(f\"QUICK SUMMARY\")\n","        print(f\"{'='*60}\")\n","\n","        print(f\"{'Property':<8} {'Test R²':<8} {'Test RMSE':<10} {'Test MAE':<8} {'Train/Test R²':<12}\")\n","        print(\"-\" * 60)\n","        for result in all_results:\n","            r2_ratio = result['Train_R2'] / result['Test_R2'] if result['Test_R2'] > 0 else np.inf\n","            print(f\"{result['Property']:<8} {result['Test_R2']:<8.3f} {result['Test_RMSE']:<10.2f} \"\n","                  f\"{result['Test_MAE']:<8.2f} {r2_ratio:<12.2f}\")\n","\n","        # Best and worst performing properties\n","        best_r2 = max(all_results, key=lambda x: x['Test_R2'])\n","        worst_r2 = min(all_results, key=lambda x: x['Test_R2'])\n","\n","        print(f\"\\nBest R² performance: {best_r2['Property']} (R² = {best_r2['Test_R2']:.3f})\")\n","        print(f\"Worst R² performance: {worst_r2['Property']} (R² = {worst_r2['Test_R2']:.3f})\")\n","\n","        print(f\"\\nAll results saved to: {outputDir}all_properties_summary.csv\")\n","\n","    print(f\"\\n{'='*100}\")\n","    print(\" FINISHED\")\n","    print(f\"{'='*100}\")"],"metadata":{"id":"hDhWvTOxLkar"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q37eS-q-YvQ0"},"source":["### GP method without outliers"]},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Script to train a GP on physicochemical properties.\n","\"\"\"\n","\n","\n","import os\n","import warnings\n","import time\n","\n","# Specific\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from skmultilearn.model_selection import iterative_train_test_split\n","import gpflow\n","from gpflow.utilities import print_summary, set_trainable, deepcopy\n","import tensorflow as tf\n","from tensorflow_probability import bijectors as tfb\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","# =============================================================================\n","# Auxiliary Functions\n","# =============================================================================\n","\n","\n","def set_white_exp_95CI(code):\n","    \"\"\"\n","    Sets the estimated average ~95% confidence interval on labels using the property code\n","\n","    Parameters:\n","    code : string\n","        Property code\n","\n","    Returns:\n","    exp_95CI : float\n","    \"\"\"\n","    if code == 'Tb':\n","        exp_95CI = 10.0\n","    elif code == 'Tm':\n","        exp_95CI = 5.0\n","    elif code == 'Hvap':\n","        exp_95CI = 1.0\n","    elif code == 'Vc':\n","        exp_95CI = 25.0\n","    elif code == 'Tc':\n","        exp_95CI = 5.0\n","    elif code == 'Pc':\n","        exp_95CI = 30.0\n","    elif code == 'logP':\n","        exp_95CI = 1.0\n","    return exp_95CI\n","\n","\n","\n","def gpConfig_from_method(method_number, code, kernel = 'RBF', anisotropic = False, useWhiteKernel = True, trainLikelihood = True, opt_method = 'L-BFGS-B'):\n","    \"\"\"\n","    Creates a gpConfig dictionary based on the method number.\n","\n","    Parameters:\n","    method_number : int\n","        Method number.\n","\n","    Returns:\n","    gpConfig : dictionary\n","        Dictionary of GP configuration parameters.\n","\n","    Note:\n","    method_number is used to define which type of gp model to use\n","    1: Y = GP(0, K(Mw, Y_gc))\n","    2: Y - Y_gc = GP(0, K(Mw))\n","    3: Y - Y_gc = GP(0, K(Mw, Y_gc))\n","    4: Y = GP(Y_gc, K(Mw, Y_gc))\n","    5: Y = GP(AMw + BY_gc + c, K(Mw, Y_gc))\n","    \"\"\"\n","    gpConfig={'kernel': kernel,\n","           'useWhiteKernel':useWhiteKernel,\n","           'trainLikelihood':trainLikelihood,\n","           'opt_method':opt_method,\n","           'anisotropic':anisotropic}\n","    if method_number == 1:\n","        gpConfig['mean_function']='Zero'\n","        gpConfig['Name']='y_exp = GP(0, K(x1,x2))'\n","        gpConfig['SaveName']='model_1'\n","    if method_number == 2:\n","        gpConfig['mean_function']='Zero'\n","        gpConfig['Name']='y_exp = y_GC + GP(0, K(x1))'\n","        gpConfig['SaveName']='model_2'\n","    if method_number == 3:\n","        gpConfig['mean_function']='Constant'\n","        gpConfig['Name']='y_exp = GP(y_GC, K(x1,x2))'\n","        gpConfig['SaveName']='model_3'\n","    if method_number == 4:\n","        gpConfig['mean_function']='Linear'\n","        gpConfig['Name']='y_exp = GP(B@X, K(x1,x2))'\n","        gpConfig['SaveName']='model_4'\n","    else:\n","        if method_number not in [1 , 2, 3, 4]:\n","            raise ValueError('invalid method number input')\n","    return gpConfig\n","\n","\n","def get_gp_data(X, Y, method_number):\n","    \"\"\"\n","    Gets X and Y data to train GP based on the method number\n","\n","    Parameters:\n","    X : numpy array\n","        Features data.\n","    Y : numpy array\n","        Property data.\n","    method_number : int\n","        Method number\n","\n","    Returns:\n","    X_gp : numpy array\n","        Features data to train GP.\n","    Y_gp: numpy array\n","        Data to train GP.\n","    Y_gc: numpy array\n","        Data from Joback method\n","    \"\"\"\n","    if method_number == 2:\n","        X_gp = X[:,0].reshape(-1,1)\n","    else:\n","        X_gp = X\n","    if method_number == 2:\n","        Y_gp = Y - X[:,1]\n","    else:\n","        Y_gp = Y\n","    Y_gp = Y_gp.reshape(-1,1)\n","    Y_gc = X[:,1].reshape(-1,1)\n","    return X_gp, Y_gp, Y_gc\n","\n","\n","def discrepancy_to_property(method_number, y_pred, y_gc, idx):\n","    \"\"\"\n","    Adds discrepancy to property based on the method number\n","    Parameters:\n","    method_number : int\n","        Method number\n","    y_pred : numpy array\n","        GP predicted output\n","    y_gc : numpy array\n","        Predicted GC method results\n","    idx : np.array\n","        Index of the y_gc to be added to y_pred\n","\n","    Returns:\n","    y_prop : numpy array\n","        Predicted property value\n","    \"\"\"\n","    if method_number == 2:\n","        y_prop = y_pred + y_gc[idx.flatten(),:]\n","    else:\n","        y_prop = y_pred\n","    return y_prop\n","\n","def stratifyvector(Y):\n","    \"\"\"\n","    Creates a stratified vector based on the label data Y\n","\n","    Parameters:\n","    Y : numpy array\n","        label data\n","    Returns:\n","    stratifyVector : numpy array\n","        Stratified vector\n","    \"\"\"\n","    # Iterate over number of bins, trying to find the larger number of bins that\n","    # guarantees at least 5 values per bin\n","    for n in range(1,100):\n","        # Bin Y using n bins\n","        stratifyVector=pd.cut(Y,n,labels=False)\n","        # Define isValid (all bins have at least 5 values)\n","        isValid=True\n","        # Check that all bins have at least 5 values\n","        for k in range(n):\n","            if np.count_nonzero(stratifyVector==k)<5:\n","                isValid=False\n","        #If isValid is false, n is too large; nBins must be the previous iteration\n","        if not isValid:\n","            nBins=n-1\n","            break\n","    # Generate vector for stratified splitting based on labels\n","    stratifyVector=pd.cut(Y,nBins,labels=False)\n","    return stratifyVector\n","\n","def normalize(inputArray,skScaler=None,method='Standardization',reverse=False):\n","    \"\"\"\n","    normalize() normalizes (or unnormalizes) inputArray using the method\n","    specified and the skScaler provided.\n","\n","    Parameters\n","    ----------\n","    inputArray : numpy array\n","        Array to be normalized. If dim>1, array is normalized column-wise.\n","    skScaler : scikit-learn preprocessing object or None\n","        Scikit-learn preprocessing object previosly fitted to data. If None,\n","        the object is fitted to inputArray.\n","        Default: None\n","    method : string, optional\n","        Normalization method to be used.\n","        Methods available:\n","            . Standardization - classic standardization, (x-mean(x))/std(x)\n","            . MinMax - scale to range (0,1)\n","            . LogStand - standardization on the log of the variable,\n","                         (log(x)-mean(log(x)))/std(log(x))\n","            . Log+bStand - standardization on the log of variables that can be\n","                           zero; uses a small buffer,\n","                           (log(x+b)-mean(log(x+b)))/std(log(x+b))\n","        Default: 'Standardization'\n","    reverse : bool\n","        Whether  to normalize (False) or unnormalize (True) inputArray.\n","        Defalt: False\n","\n","    Returns\n","    -------\n","    inputArray : numpy array\n","        Normalized (or unnormalized) version of inputArray.\n","    skScaler : scikit-learn preprocessing object\n","        Scikit-learn preprocessing object fitted to inputArray. It is the same\n","        as the inputted skScaler, if it was provided.\n","\n","    \"\"\"\n","    # If inputArray is a labels vector of size (N,), reshape to (N,1)\n","    if inputArray.ndim==1:\n","        inputArray=inputArray.reshape((-1,1))\n","        warnings.warn('Input to normalize() was of shape (N,). It was assumed'\\\n","                      +' to be a column array and converted to a (N,1) shape.')\n","    # If skScaler is None, train for the first time\n","    if method == 'None':\n","        inputArray=inputArray\n","        skScaler = None\n","    else:\n","        if skScaler is None:\n","            # Check method\n","            if method=='Standardization' or method=='MinMax': aux=inputArray\n","            elif method=='LogStand': aux=np.log(inputArray)\n","            elif method=='Log+bStand': aux=np.log(inputArray+10**-3)\n","            else: raise ValueError('Could not recognize method in normalize().')\n","            if method!='MinMax':\n","                skScaler=preprocessing.StandardScaler().fit(aux)\n","            else:\n","                skScaler=preprocessing.MinMaxScaler().fit(aux)\n","        # Do main operation (normalize or unnormalize)\n","        if reverse:\n","            # Rescale the data back to its original distribution\n","            inputArray=skScaler.inverse_transform(inputArray)\n","            # Check method\n","            if method=='LogStand': inputArray=np.exp(inputArray)\n","            elif method=='Log+bStand': inputArray=np.exp(inputArray)-10**-3\n","        elif not reverse:\n","            # Check method\n","            if method=='Standardization' or method=='MinMax': aux=inputArray\n","            elif method=='LogStand': aux=np.log(inputArray)\n","            elif method=='Log+bStand': aux=np.log(inputArray+10**-3)\n","            else: raise ValueError('Could not recognize method in normalize().')\n","            inputArray=skScaler.transform(aux)\n","    # Return\n","    return inputArray,skScaler\n","\n","\n","\n","# Build GPR model function with bounded hyperparameters\n","def build_model_with_bounded_params(X, Y, kern, low, high, \\\n","                                    high_alpha, init_val1, init_val2, init_val3, \\\n","                                    useWhite, trainLikelihood, anisotropic, typeMeanFunc):\n","    \"\"\"\n","    build_model_with_bounded_params(*) creates a GP model object with bounded hyperparameters and initial\n","    values\n","\n","    Parameters\n","    ----------\n","    X : numpy array\n","        Feature data\n","    Y : numpy array\n","        Label data\n","    low : float\n","        lower bound on all hyperparameters\n","    high : float\n","        upper bound on all hyperparameters except alpha for the RQ kernel\n","    high_alpha : float\n","        upper bound on alpha hyperparameter for the RQ kernel\n","    init_val1 : float\n","        initial values for first length scale and alpha parameter\n","    init_val2 : float\n","        initial values for second length scale for anisotropic kernels\n","        for isotropic kernels, only one initial value (init_val1) is used\n","    init_val1 : float\n","        initial values for variance or scale hyperparameter of kernel 1 (not Whitenoise kernel)\n","\n","    Returns\n","    -------\n","    model : Gpflow model object\n","        GP model object with bounded hyperparameters and initial values\n","\n","    \"\"\"\n","\n","    low = tf.cast(low, dtype=tf.float64)\n","    high = tf.cast(high, dtype=tf.float64)\n","    high_alpha = tf.cast(high_alpha, dtype=tf.float64)\n","    init_val1 = tf.cast(init_val1, dtype=tf.float64)\n","    init_val2 = tf.cast(init_val2, dtype=tf.float64)\n","    init_val3 = tf.cast(init_val3, dtype=tf.float64)\n","    if anisotropic == True:\n","        lsc = gpflow.Parameter([init_val1, init_val2], transform=tfb.Sigmoid(low , high), dtype=tf.float64)\n","    else:\n","        lsc = gpflow.Parameter(init_val1, transform=tfb.Sigmoid(low , high), dtype=tf.float64)\n","    alf = gpflow.Parameter(init_val1, transform=tfb.Sigmoid(low , high_alpha), dtype=tf.float64)\n","    var = gpflow.Parameter(init_val3, transform=tfb.Sigmoid(low , high), dtype=tf.float64)\n","    if kern == \"RQ\":\n","        kernel_ = gpflow.kernels.RationalQuadratic()\n","        kernel_.alpha = alf\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    elif kern == \"RBF\":\n","        kernel_ = gpflow.kernels.RBF()\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    elif kern == \"Matern12\":\n","        kernel_ = gpflow.kernels.Matern12()\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    elif kern == \"Matern32\":\n","        kernel_ = gpflow.kernels.Matern32()\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    elif kern == \"Matern52\":\n","        kernel_ = gpflow.kernels.Matern52()\n","        kernel_.lengthscales = lsc\n","        kernel_.variance = var\n","    if useWhite == True:\n","        #white_var = np.array(np.random.uniform(0.05, 1.0))\n","        final_kernel = kernel_+gpflow.kernels.White(variance=1.0)\n","    else:\n","        final_kernel = kernel_\n","\n","    if typeMeanFunc == 'Zero':\n","        mf = None\n","    if typeMeanFunc == 'Constant':\n","        #If constant value is selected but no value is given, default to zero mean\n","        mf_val = np.array([0,1]).reshape(-1,1)\n","        mf = gpflow.functions.Linear(mf_val)\n","    if typeMeanFunc == 'Linear':\n","        A = np.ones((X.shape[1],1))\n","        mf = gpflow.functions.Linear(A)\n","    model_ = gpflow.models.GPR(data=(X, Y), kernel=final_kernel, mean_function=mf, noise_variance=10**-5)\n","    if typeMeanFunc == 'Constant':\n","        gpflow.set_trainable(model_.mean_function.A, False)\n","        gpflow.set_trainable(model_.mean_function.b, False)\n","    gpflow.utilities.set_trainable(model_.likelihood.variance,trainLikelihood)\n","    model = model_\n","    return model\n","\n","\n","\n","def buildGP(X_Train, Y_Train, gpConfig, code, featurenorm, retrain_count):\n","    \"\"\"\n","    buildGP() builds and fits a GP model using the training data provided.\n","\n","    Parameters\n","    ----------\n","    X_Train : numpy array (N,K)\n","        Training features, where N is the number of data points and K is the\n","        number of independent features\n","    Y_Train : numpy array (N,1)\n","        Training labels (e.g., property of a given molecule).\n","    gpConfig : dictionary, optional\n","        Dictionary containing the configuration of the GP. If a key is not\n","        present in the dictionary, its default value is used.\n","        Keys:\n","            . kernel : string\n","                Kernel to be used. One of:\n","                    . 'RBF' - gpflow.kernels.RBF()\n","                    . 'RQ' - gpflow.kernels.RationalQuadratic()\n","                    . 'Matern12' - gpflow.kernels.Matern12()\n","                    . 'Matern32' - gpflow.kernels.Matern32()\n","                    . 'Matern52' - gpflow.kernels.Matern52()\n","                The default is 'RQ'.\n","            . useWhiteKernel : boolean\n","                Whether to use a White kernel (gpflow.kernels.White).\n","                The default is True.\n","            . trainLikelihood : boolean\n","                Whether to treat the variance of the likelihood of the modeal\n","                as a trainable (or fitting) parameter. If False, this value is\n","                fixed at 10^-5.\n","                The default is True.\n","        The default is {}.\n","    sc_y_scale : Scikit learn standard scaler object\n","        standard scaler fitted on label training data\n","    retrain_count : int\n","        Current GP retrain number\n","\n","    \"\"\"\n","    # Unpack gpConfig\n","    kernel=gpConfig.get('kernel','RQ')\n","    useWhiteKernel=gpConfig.get('useWhiteKernel','True')\n","    trainLikelihood=gpConfig.get('trainLikelihood','True')\n","    typeMeanFunc=gpConfig.get('mean_function','Zero')\n","    opt_method=gpConfig.get('opt_method','L-BFGS-B')\n","    anisotropy=gpConfig.get('anisotropic','False')\n","\n","    seed_ = int(retrain_count) * 100\n","    np.random.seed(seed_)\n","    tf.random.set_seed(seed_)\n","\n","    if retrain_count == 0:\n","        init_val1 = 1\n","        init_val2 = 1\n","        init_val3 = 1\n","    else:\n","        init_val1 = np.array(np.random.uniform(0, 100))\n","        init_val2 = np.array(np.random.uniform(0, 100))\n","        init_val3 = np.array(np.random.lognormal(0, 1.0))\n","\n","    model = build_model_with_bounded_params(X_Train, Y_Train, kernel, 0.00001, 100, 5000, init_val1, \\\n","                            init_val2, init_val3, useWhiteKernel, trainLikelihood, anisotropy, typeMeanFunc)\n","    model_pretrain = deepcopy(model)\n","    # print(gpflow.utilities.print_summary(model))\n","    condition_number = np.linalg.cond(model.kernel(X_Train))\n","    # Build optimizer\n","    optimizer=gpflow.optimizers.Scipy()\n","    # Fit GP to training data\n","    aux=optimizer.minimize(model.training_loss,\n","                           model.trainable_variables,\n","                           options={'maxiter':10**9},\n","                           method=opt_method)\n","    obj_func = model.training_loss()\n","    if aux.success:\n","        opt_success = True\n","    else:\n","        opt_success = False\n","\n","    return model, aux, condition_number, obj_func, opt_success, retrain_count, model_pretrain\n","\n","\n","\n","def train_gp(X_Train, Y_Train, gpConfig, code, sc_y, featurenorm, retrain_GP, retrain_count):\n","    \"\"\"\n","    Trains the GP given training data.\n","\n","\n","    \"\"\"\n","\n","    # Train the model multiple times and keep track of the model with the lowest minimum training loss\n","    best_minimum_loss = float('inf')\n","    best_model = None\n","    best_model_pretrain = None\n","    best_model_success = False\n","    best_condition_num = float('inf')\n","    args = (X_Train, Y_Train, gpConfig)\n","\n","    retrain_GP = int(retrain_GP)\n","    retrain_count = retrain_count\n","    for i in range(retrain_GP):\n","        model, aux, condition_number, obj_func, opt_success, retrain_count, model_pretrain = \\\n","            buildGP(X_Train, Y_Train, gpConfig, code, featurenorm, retrain_count)\n","        print(f\"training_loss = {obj_func}\")\n","        print(f\"condition_number = {condition_number}\")\n","        retrain_count += 1\n","        if best_minimum_loss > obj_func and opt_success==True:\n","            best_minimum_loss = obj_func\n","            best_model = model\n","            best_model_pretrain = model_pretrain\n","            best_model_success = opt_success\n","            best_condition_num = condition_number\n","    if best_model_success == False:\n","        warnings.warn('GP optimizer failed to converge with retrains')\n","\n","    #Put hyperparameters in a list\n","    trained_hyperparams = gpflow.utilities.read_values(best_model)\n","\n","    if sc_y != None:\n","        sc_y_scale = sc_y.scale_\n","    else:\n","        sc_y_scale = None\n","\n","    return best_model,best_minimum_loss,best_model_success,best_condition_num,trained_hyperparams,best_model_pretrain,sc_y_scale\n","\n","\n","def gpPredict(model,X):\n","    \"\"\"\n","    gpPredict() returns the prediction and variance of the GP model\n","    on the X data provided.\n","\n","    Parameters\n","    ----------\n","    model : gpflow.models.gpr.GPR object\n","        GP model.\n","    X : numpy array (N,K)\n","        Training features, where N is the number of data points and K is the\n","        number of independent features (e.g., sigma profile bins).\n","\n","    Returns\n","    -------\n","    Y : numpy array (N,1)\n","        GP predictions.\n","    STD : numpy array (N,1)\n","        GP standard deviations.\n","\n","    \"\"\"\n","    # Do GP prediction, obtaining mean and variance\n","    GP_Mean,GP_Var=model.predict_f(X)\n","    # Convert to numpy\n","    GP_Mean=GP_Mean.numpy()\n","    GP_Var=GP_Var.numpy()\n","    # Prepare outputs\n","    Y=GP_Mean\n","    VAR=GP_Var\n","    # Output\n","    return Y,VAR\n","\n","\n","\n","\n","def count_outside_95(Y_Train, Y_Test, Y_Train_Pred, Y_Test_Pred, Y_Train_CI, Y_Test_CI):\n","    \"\"\"\n","    count_outside_95() finds the number and fraction of predicted data that are outside the predicted 95%\n","        confidence intervals from the true values\n","\n","    Parameters:\n","    Y_Train_CI : numpy array\n","        Absolute values of the 95% confidence interval on the predictions on training set\n","    Y_Test_CI : numpy array\n","        Absolute values of the 95% confidence interval on the predictions on testing set\n","\n","    \"\"\"\n","    out_95_train = []\n","    out_95_test = []\n","    for index, value in enumerate(Y_Train):\n","        if np.abs(value - Y_Train_Pred[index]) > Y_Train_CI[index]:\n","            out_95_train.append(index)\n","    num_out95_train = len(out_95_train)\n","    frac_out95_train = num_out95_train/len(Y_Train)\n","    for index, value in enumerate(Y_Test):\n","        if np.abs(value - Y_Test_Pred[index]) > Y_Test_CI[index]:\n","            out_95_test.append(index)\n","    num_out95_test = len(out_95_test)\n","    frac_out95_test = num_out95_test/len(Y_Test)\n","\n","    return num_out95_train, frac_out95_train, num_out95_test, frac_out95_test\n","\n","\n","# =============================================================================\n","# Statistical Metrics and Plotting Functions\n","# =============================================================================\n","def calculate_metrics(y_true, y_pred):\n","    \"\"\"Calculate comprehensive statistical metrics\"\"\"\n","    y_true = np.array(y_true).flatten()\n","    y_pred = np.array(y_pred).flatten()\n","\n","    mae = mean_absolute_error(y_true, y_pred)\n","    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n","    r2 = r2_score(y_true, y_pred)\n","\n","    # MAPE and MPE with zero-division protection\n","    mask = y_true != 0\n","    if mask.sum() > 0:\n","        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n","        mpe = np.mean((y_true[mask] - y_pred[mask]) / y_true[mask]) * 100\n","    else:\n","        mape = np.inf\n","        mpe = np.inf\n","\n","    return {'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape, 'MPE': mpe}\n","\n","def calculate_improvement(train_metrics, test_metrics):\n","    \"\"\"Calculate improvement metrics (positive values indicate overfitting)\"\"\"\n","    improvement = {}\n","\n","    # For R2: higher is better, so improvement = train - test\n","    improvement['R2'] = train_metrics['R2'] - test_metrics['R2']\n","\n","    # For error metrics: lower is better, so improvement = test - train\n","    improvement['MAE'] = test_metrics['MAE'] - train_metrics['MAE']\n","    improvement['RMSE'] = test_metrics['RMSE'] - train_metrics['RMSE']\n","    improvement['MAPE'] = test_metrics['MAPE'] - train_metrics['MAPE']\n","    improvement['MPE'] = test_metrics['MPE'] - train_metrics['MPE']\n","\n","    return improvement\n","\n","def plot_predictions(Y_train_true, Y_train_pred, Y_test_true, Y_test_pred,\n","                    Y_train_ci, Y_test_ci, property_name, save_path):\n","    \"\"\"Create side-by-side parity plots: Train, Test, and Combined\"\"\"\n","\n","    # Set style\n","    plt.style.use('default')\n","    sns.set_palette(\"husl\")\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n","    fig.suptitle(f'GP Predictions for {property_name}', fontsize=16, fontweight='bold')\n","\n","    # Flatten arrays\n","    y_train_true_flat = Y_train_true.flatten()\n","    y_train_pred_flat = Y_train_pred.flatten()\n","    y_train_ci_flat = Y_train_ci.flatten()\n","    y_test_true_flat = Y_test_true.flatten()\n","    y_test_pred_flat = Y_test_pred.flatten()\n","    y_test_ci_flat = Y_test_ci.flatten()\n","\n","    # Calculate metrics\n","    train_metrics = calculate_metrics(y_train_true_flat, y_train_pred_flat)\n","    test_metrics = calculate_metrics(y_test_true_flat, y_test_pred_flat)\n","\n","    # Combined data for the third plot\n","    y_combined_true = np.concatenate([y_train_true_flat, y_test_true_flat])\n","    y_combined_pred = np.concatenate([y_train_pred_flat, y_test_pred_flat])\n","    y_combined_ci = np.concatenate([y_train_ci_flat, y_test_ci_flat])\n","    combined_metrics = calculate_metrics(y_combined_true, y_combined_pred)\n","\n","    # Plot 1: Training Set\n","    ax1 = axes[0]\n","    # Changed: True on x-axis, Predicted on y-axis\n","    ax1.scatter(y_train_true_flat, y_train_pred_flat, alpha=0.6, s=30, color='blue')\n","    ax1.errorbar(y_train_true_flat, y_train_pred_flat, yerr=y_train_ci_flat,\n","                fmt='none', alpha=0.3, color='blue')\n","\n","    min_val = min(y_train_true_flat.min(), y_train_pred_flat.min())\n","    max_val = max(y_train_true_flat.max(), y_train_pred_flat.max())\n","    ax1.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, linewidth=2)\n","\n","    ax1.set_title(f'Training Set (n={len(y_train_true_flat)})\\n'\n","                  f'R² = {train_metrics[\"R2\"]:.3f}, RMSE = {train_metrics[\"RMSE\"]:.2f}\\n'\n","                  f'MAE = {train_metrics[\"MAE\"]:.2f}, MAPE = {train_metrics[\"MAPE\"]:.1f}%')\n","    ax1.set_xlabel(f'True {property_name}')        # Changed: True on x-axis\n","    ax1.set_ylabel(f'Predicted {property_name}')   # Changed: Predicted on y-axis\n","    ax1.grid(True, alpha=0.3)\n","\n","    # Plot 2: Test Set\n","    ax2 = axes[1]\n","    # Changed: True on x-axis, Predicted on y-axis\n","    ax2.scatter(y_test_true_flat, y_test_pred_flat, alpha=0.6, s=30, color='red')\n","    ax2.errorbar(y_test_true_flat, y_test_pred_flat, yerr=y_test_ci_flat,\n","                fmt='none', alpha=0.3, color='red')\n","\n","    min_val = min(y_test_true_flat.min(), y_test_pred_flat.min())\n","    max_val = max(y_test_true_flat.max(), y_test_pred_flat.max())\n","    ax2.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, linewidth=2)\n","\n","    ax2.set_title(f'Test Set (n={len(y_test_true_flat)})\\n'\n","                  f'R² = {test_metrics[\"R2\"]:.3f}, RMSE = {test_metrics[\"RMSE\"]:.2f}\\n'\n","                  f'MAE = {test_metrics[\"MAE\"]:.2f}, MAPE = {test_metrics[\"MAPE\"]:.1f}%')\n","    ax2.set_xlabel(f'True {property_name}')        # Changed: True on x-axis\n","    ax2.set_ylabel(f'Predicted {property_name}')   # Changed: Predicted on y-axis\n","    ax2.grid(True, alpha=0.3)\n","\n","    # Plot 3: Combined Train + Test Set\n","    ax3 = axes[2]\n","    # Changed: True on x-axis, Predicted on y-axis\n","    ax3.scatter(y_train_true_flat, y_train_pred_flat, alpha=0.5, s=25, color='blue', label='Training')\n","    ax3.scatter(y_test_true_flat, y_test_pred_flat, alpha=0.5, s=25, color='red', label='Test')\n","    ax3.errorbar(y_combined_true, y_combined_pred, yerr=y_combined_ci,\n","                fmt='none', alpha=0.2, color='gray')\n","\n","    min_val = min(y_combined_true.min(), y_combined_pred.min())\n","    max_val = max(y_combined_true.max(), y_combined_pred.max())\n","    ax3.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, linewidth=2)\n","\n","    ax3.set_title(f'Combined (n={len(y_combined_true)})\\n'\n","                  f'R² = {combined_metrics[\"R2\"]:.3f}, RMSE = {combined_metrics[\"RMSE\"]:.2f}\\n'\n","                  f'MAE = {combined_metrics[\"MAE\"]:.2f}, MAPE = {combined_metrics[\"MAPE\"]:.1f}%')\n","    ax3.set_xlabel(f'True {property_name}')        # Changed: True on x-axis\n","    ax3.set_ylabel(f'Predicted {property_name}')   # Changed: Predicted on y-axis\n","    ax3.grid(True, alpha=0.3)\n","    ax3.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"{save_path}/{property_name}_GP_results.png\", dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","    return train_metrics, test_metrics, combined_metrics\n","\n","\n","\n","# =============================================================================\n","# Outlier Detection and Removal Functions\n","# =============================================================================\n","def detect_outliers_iqr(data, multiplier=1.5):\n","    \"\"\"\n","    Detect outliers using Interquartile Range (IQR) method\n","\n","    Parameters:\n","    data : numpy array\n","        1D array of data values\n","    multiplier : float\n","        IQR multiplier (1.5 is standard, 3.0 is more conservative)\n","\n","    Returns:\n","    outlier_mask : numpy array\n","        Boolean mask where True indicates outlier\n","    \"\"\"\n","    Q1 = np.percentile(data, 25)\n","    Q3 = np.percentile(data, 75)\n","    IQR = Q3 - Q1\n","    lower_bound = Q1 - multiplier * IQR\n","    upper_bound = Q3 + multiplier * IQR\n","\n","    outlier_mask = (data < lower_bound) | (data > upper_bound)\n","    return outlier_mask\n","\n","def detect_outliers_zscore(data, threshold=3.0):\n","    \"\"\"\n","    Detect outliers using Z-score method\n","\n","    Parameters:\n","    data : numpy array\n","        1D array of data values\n","    threshold : float\n","        Z-score threshold (3.0 is standard)\n","\n","    Returns:\n","    outlier_mask : numpy array\n","        Boolean mask where True indicates outlier\n","    \"\"\"\n","    z_scores = np.abs((data - np.mean(data)) / np.std(data))\n","    outlier_mask = z_scores > threshold\n","    return outlier_mask\n","\n","def detect_outliers_modified_zscore(data, threshold=3.5):\n","    \"\"\"\n","    Detect outliers using Modified Z-score method (more robust)\n","\n","    Parameters:\n","    data : numpy array\n","        1D array of data values\n","    threshold : float\n","        Modified Z-score threshold (3.5 is standard)\n","\n","    Returns:\n","    outlier_mask : numpy array\n","        Boolean mask where True indicates outlier\n","    \"\"\"\n","    median = np.median(data)\n","    mad = np.median(np.abs(data - median))\n","    modified_z_scores = 0.6745 * (data - median) / mad\n","    outlier_mask = np.abs(modified_z_scores) > threshold\n","    return outlier_mask\n","\n","def remove_outliers(X, Y, method='iqr', threshold=1.5, verbose=True):\n","    \"\"\"\n","    Remove outliers from the dataset based on target variable Y\n","\n","    Parameters:\n","    X : numpy array\n","        Feature matrix\n","    Y : numpy array\n","        Target values\n","    method : str\n","        Outlier detection method ('iqr', 'zscore', 'modified_zscore')\n","    threshold : float\n","        Threshold parameter for outlier detection\n","    verbose : bool\n","        Whether to print outlier removal statistics\n","\n","    Returns:\n","    X_clean : numpy array\n","        Feature matrix with outliers removed\n","    Y_clean : numpy array\n","        Target values with outliers removed\n","    outlier_info : dict\n","        Information about outlier removal\n","    \"\"\"\n","    Y_flat = Y.flatten()\n","\n","    # Detect outliers\n","    if method == 'iqr':\n","        outlier_mask = detect_outliers_iqr(Y_flat, multiplier=threshold)\n","    elif method == 'zscore':\n","        outlier_mask = detect_outliers_zscore(Y_flat, threshold=threshold)\n","    elif method == 'modified_zscore':\n","        outlier_mask = detect_outliers_modified_zscore(Y_flat, threshold=threshold)\n","    else:\n","        raise ValueError(f\"Unknown outlier detection method: {method}\")\n","\n","    # Create clean datasets\n","    clean_mask = ~outlier_mask\n","    X_clean = X[clean_mask]\n","    Y_clean = Y[clean_mask]\n","\n","    # Calculate statistics\n","    n_original = len(Y_flat)\n","    n_outliers = np.sum(outlier_mask)\n","    n_clean = len(Y_clean)\n","    outlier_percentage = (n_outliers / n_original) * 100\n","\n","    outlier_info = {\n","        'method': method,\n","        'threshold': threshold,\n","        'n_original': n_original,\n","        'n_outliers': n_outliers,\n","        'n_clean': n_clean,\n","        'outlier_percentage': outlier_percentage,\n","        'outlier_indices': np.where(outlier_mask)[0],\n","        'outlier_values': Y_flat[outlier_mask]\n","    }\n","\n","    if verbose:\n","        print(f\"Outlier removal ({method}, threshold={threshold}):\")\n","        print(f\"  Original samples: {n_original}\")\n","        print(f\"  Outliers detected: {n_outliers} ({outlier_percentage:.1f}%)\")\n","        print(f\"  Clean samples: {n_clean}\")\n","        if n_outliers > 0:\n","            print(f\"  Outlier range: [{np.min(Y_flat[outlier_mask]):.2f}, {np.max(Y_flat[outlier_mask]):.2f}]\")\n","\n","    return X_clean, Y_clean, outlier_info\n","\n","def process_single_property(code, kernel, anisotropic, opt_method, useWhiteKernel,\n","                          trainLikelihood, retrain_GP, method_number, outputDir,\n","                          modelBuildingDataDir, seed=42,\n","                          remove_outliers_flag=True, outlier_method='iqr', outlier_threshold=1.5):\n","    \"\"\"\n","    Process a single property code with optional outlier removal\n","\n","    Parameters:\n","    -----------\n","    code : str\n","        Property code (e.g., 'Tb', 'Tm', etc.)\n","    kernel : str\n","        Kernel type for GP\n","    anisotropic : bool\n","        Whether to use anisotropic kernel\n","    opt_method : str\n","        Optimization method\n","    useWhiteKernel : bool\n","        Whether to use white noise kernel\n","    trainLikelihood : bool\n","        Whether to train likelihood variance\n","    retrain_GP : int\n","        Number of GP retraining attempts\n","    method_number : int\n","        Method number for GP configuration\n","    outputDir : str\n","        Output directory path\n","    modelBuildingDataDir : str\n","        Model building data directory path\n","    seed : int\n","        Random seed for reproducibility\n","    remove_outliers_flag : bool\n","        Whether to remove outliers before training\n","    outlier_method : str\n","        Outlier detection method ('iqr', 'zscore', 'modified_zscore')\n","    outlier_threshold : float\n","        Threshold for outlier detection\n","\n","    Returns:\n","    --------\n","    dict or None\n","        Results summary dictionary or None if processing failed\n","    \"\"\"\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"PROCESSING PROPERTY: {code}\")\n","    print(f\"{'='*80}\")\n","\n","    # Set random seed for reproducibility\n","    np.random.seed(seed)\n","\n","    # Define normalization methods\n","    if method_number == 2:\n","        featureNorm, labelNorm = 'None', 'None'\n","    else:\n","        featureNorm, labelNorm = 'Standardization', 'Standardization'\n","\n","    # GP Configuration\n","    gpConfig = gpConfig_from_method(method_number, code, kernel, anisotropic, useWhiteKernel, trainLikelihood, opt_method)\n","\n","    try:\n","        # =============================================================================\n","        # Data Loading\n","        # =============================================================================\n","        data_file = f\"{modelBuildingDataDir}{code}_prediction_data_fcl_with_N.csv\"\n","        if not os.path.exists(data_file):\n","            print(f\"Warning: Data file not found for {code}: {data_file}\")\n","            return None\n","\n","        db = pd.read_csv(data_file)\n","        print(f\"Original data shape: {db.shape}\")\n","        # Only drop rows where feature columns (2:-1) or target column (-1) have NaN\n","        feature_target_cols = db.columns[2:]  # Features + target columns\n","        db = db.dropna(subset=feature_target_cols)\n","        print(f\"After removing rows with NaN in features/target: {db.shape}\")\n","        X_original = db.iloc[:,2:-1].copy().to_numpy('float')\n","        data_names = db.columns.tolist()[2:]\n","        Y_original = db.iloc[:,-1].copy().to_numpy('float')\n","        Y_original = Y_original.reshape(-1,1)\n","\n","        print(f\"Loaded data for {code}: {X_original.shape[0]} samples, {X_original.shape[1]} features\")\n","\n","        # Store original data info\n","        n_original = len(Y_original)\n","\n","        # =============================================================================\n","        # Outlier Detection and Removal\n","        # =============================================================================\n","        outlier_info = None\n","        if remove_outliers_flag:\n","            print(f\"\\nApplying outlier removal using {outlier_method} method (threshold={outlier_threshold})...\")\n","\n","            # Apply outlier removal\n","            X_clean, Y_clean, outlier_info = remove_outliers(\n","                X_original, Y_original,\n","                method=outlier_method,\n","                threshold=outlier_threshold,\n","                verbose=True\n","            )\n","\n","            # Update dataframe to reflect outlier removal\n","            clean_mask = ~detect_outliers_iqr(Y_original.flatten(), multiplier=outlier_threshold) if outlier_method == 'iqr' else \\\n","                        ~detect_outliers_zscore(Y_original.flatten(), threshold=outlier_threshold) if outlier_method == 'zscore' else \\\n","                        ~detect_outliers_modified_zscore(Y_original.flatten(), threshold=outlier_threshold)\n","\n","            db_clean = db[clean_mask].reset_index(drop=True)\n","\n","            # Use cleaned data\n","            X = X_clean\n","            Y = Y_clean\n","            db = db_clean\n","\n","            print(f\"After outlier removal: {X.shape[0]} samples remaining\")\n","\n","        else:\n","            print(f\"Outlier removal disabled - using all {n_original} samples\")\n","            X = X_original\n","            Y = Y_original\n","            outlier_info = {\n","                'method': 'None',\n","                'threshold': 'N/A',\n","                'n_original': n_original,\n","                'n_outliers': 0,\n","                'n_clean': n_original,\n","                'outlier_percentage': 0.0,\n","                'outlier_indices': [],\n","                'outlier_values': []\n","            }\n","\n","        # Extract additional variables from cleaned data\n","        Y_gc = X[:,-1].reshape(-1,1)\n","        MW = X[:,-2].reshape(-1,1)\n","\n","        # =============================================================================\n","        # Stratified Train-Test Split\n","        # =============================================================================\n","        X_data = db.iloc[:,2:-1].copy()\n","        num_rows_X = X_data.shape[0]\n","        y_data_dum = (np.ones((num_rows_X, 2))).astype(int)\n","        indices = np.arange(X_data.shape[0])\n","        y_stratify = np.column_stack((indices, y_data_dum))\n","        X_stratify = X_data.values\n","\n","        X_ = np.array(y_stratify)\n","        y_ = np.array(X_stratify)\n","        y_strat = y_\n","        X_strat = X_\n","\n","        np.random.seed(seed)\n","        X_Train_0, y_Train_0, X_valTest_0, y_valTest_0 = iterative_train_test_split(X_strat, y_strat, test_size = 0.2)\n","\n","        train_indices = (X_Train_0[:,0]).astype(int)\n","        test_indices = (X_valTest_0[:,0]).astype(int)\n","\n","        trn_idx = train_indices\n","        test_idx = test_indices\n","\n","        X_Train_0 = X[trn_idx, :]\n","        X_Test_0 = X[test_idx, :]\n","        Y_Train_0 = Y[trn_idx, :]\n","        Y_Test_0 = Y[test_idx, :]\n","\n","        # =============================================================================\n","        # Prepare GP Data Based on Method Number\n","        # =============================================================================\n","        X_Train, Y_Train, Y_gc_Train = get_gp_data(X_Train_0, Y_Train_0[:,-1], method_number)\n","        X_Test, Y_Test, Y_gc_Test = get_gp_data(X_Test_0, Y_Test_0[:,-1], method_number)\n","\n","        train_data = np.concatenate((X_Train, Y_Train), axis = 1)\n","        test_data = np.concatenate((X_Test, Y_Test), axis = 1)\n","\n","        if method_number == 2:\n","            data_names =  data_names[:1] + [data_names[-1] + \" Discrepancy\"]\n","\n","        train_df = pd.DataFrame(train_data, columns = data_names)\n","        test_df = pd.DataFrame(test_data, columns = data_names)\n","\n","        # =============================================================================\n","        # Create Output Directory and Save Data\n","        # =============================================================================\n","        property_outputDir = f\"{outputDir}{code}/{gpConfig['SaveName']}\"\n","        print(f\"Output directory: {property_outputDir}\")\n","        os.makedirs(property_outputDir, exist_ok = True)\n","\n","        train_df.to_csv(f\"{property_outputDir}/train_data.csv\", index= False)\n","        test_df.to_csv(f\"{property_outputDir}/test_data.csv\", index= False)\n","\n","        # =============================================================================\n","        # Data Normalization\n","        # =============================================================================\n","        X_Train_N = X_Train.copy()\n","        X_Test_N = X_Test.copy()\n","        Y_Train_N = Y_Train.copy()\n","        Y_gc_Train_N = Y_gc_Train.copy()\n","\n","        if featureNorm is not None:\n","            X_Train_N, skScaler_X = normalize(X_Train, method=featureNorm)\n","            X_Test_N, __ = normalize(X_Test, method=featureNorm, skScaler=skScaler_X)\n","        else:\n","            skScaler_X = None\n","\n","        if labelNorm is not None:\n","            Y_Train_N, skScaler_Y = normalize(Y_Train, method=labelNorm)\n","            Y_gc_Train_N, __ = normalize(Y_gc_Train, method=labelNorm, skScaler=skScaler_Y)\n","        else:\n","            skScaler_Y = None\n","\n","        # =============================================================================\n","        # Train Gaussian Process Model\n","        # =============================================================================\n","        print(f\"\\nTraining GP model with {retrain_GP} retraining attempts...\")\n","\n","        retrain_count = 0\n","        model, best_min_loss, fit_success, cond_num, trained_hyperparams, model_pretrain, sc_y_scale = \\\n","            train_gp(X_Train_N, Y_Train_N, gpConfig, code, skScaler_Y, featureNorm, retrain_GP, retrain_count)\n","\n","        best_lml = -1 * best_min_loss\n","        best_lml = best_lml.numpy()\n","        print(f\"Best LML: {best_lml}, Fit Success: {fit_success}, Condition Number: {cond_num}\")\n","\n","        # =============================================================================\n","        # Save Model Information\n","        # =============================================================================\n","        model_file_name = f\"{property_outputDir}/model_summary.txt\"\n","        with open(model_file_name, 'w') as file:\n","            val = gpflow.utilities.read_values(model)\n","            file.write(str(val))\n","            file.write(f\"\\n Condition Number: {cond_num}\")\n","            file.write(f\"\\n Fit Success?: {fit_success}\")\n","            file.write(f\"\\n Log-marginal Likelihood: {best_lml}\")\n","\n","        # =============================================================================\n","        # Make Predictions\n","        # =============================================================================\n","        Y_Train_Pred_N, Y_Train_Var_N = gpPredict(model, X_Train_N)\n","        Y_Test_Pred_N, Y_Test_Var_N = gpPredict(model, X_Test_N)\n","\n","        # =============================================================================\n","        # Unnormalize Predictions\n","        # =============================================================================\n","        Y_Train_Pred = Y_Train_Pred_N.copy()\n","        Y_Test_Pred = Y_Test_Pred_N.copy()\n","        Y_Train_Var = Y_Train_Var_N.copy()\n","        Y_Test_Var = Y_Test_Var_N.copy()\n","\n","        if labelNorm != 'None':\n","            Y_Train_Pred, __ = normalize(Y_Train_Pred_N, skScaler=skScaler_Y,\n","                                        method=labelNorm, reverse=True)\n","            Y_Test_Pred, __ = normalize(Y_Test_Pred_N, skScaler=skScaler_Y,\n","                                       method=labelNorm, reverse=True)\n","            Y_Train_Var = (skScaler_Y.scale_**2) * Y_Train_Var\n","            Y_Test_Var = (skScaler_Y.scale_**2) * Y_Test_Var\n","\n","        # =============================================================================\n","        # Prepare Final Results for Plotting and Analysis\n","        # =============================================================================\n","        # Convert predictions back to property values based on method\n","        if method_number == 2:\n","            Y_Test_Pred_plt = Y_Test_Pred + Y_gc_Test\n","            Y_Train_Pred_plt = Y_Train_Pred + Y_gc_Train\n","            Y_Test_plt = Y_Test + Y_gc_Test\n","            Y_Train_plt = Y_Train + Y_gc_Train\n","        else:\n","            Y_Test_Pred_plt = Y_Test_Pred\n","            Y_Train_Pred_plt = Y_Train_Pred\n","            Y_Test_plt = Y_Test\n","            Y_Train_plt = Y_Train\n","\n","        # Calculate confidence intervals\n","        Y_Test_CI_plt = 1.96 * np.sqrt(Y_Test_Var)\n","        Y_Train_CI_plt = 1.96 * np.sqrt(Y_Train_Var)\n","\n","        # =============================================================================\n","        # Calculate Confidence Interval Statistics\n","        # =============================================================================\n","        count_CI = count_outside_95(Y_Train_plt, Y_Test_plt,\n","                         Y_Train_Pred_plt, Y_Test_Pred_plt,\n","                         Y_Train_CI_plt, Y_Test_CI_plt)\n","        count_CI = np.array(count_CI)\n","\n","        # =============================================================================\n","        # Save Numerical Results\n","        # =============================================================================\n","        np.savetxt(f\"{property_outputDir}/{code}_count_CI.txt\", count_CI)\n","        np.savetxt(f\"{property_outputDir}/{code}_train_indices.txt\", trn_idx)\n","        np.savetxt(f\"{property_outputDir}/{code}_test_indices.txt\", test_idx)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_train_true.txt\", Y_Train_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_test_true.txt\", Y_Test_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_train_pred.txt\", Y_Train_Pred_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_test_pred.txt\", Y_Test_Pred_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_gc_train.txt\", Y_gc_Train)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_gc_test.txt\", Y_gc_Test)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_train_pred_95CI.txt\", Y_Train_CI_plt)\n","        np.savetxt(f\"{property_outputDir}/{code}_Y_test_pred_95CI.txt\", Y_Test_CI_plt)\n","\n","        # =============================================================================\n","        # Calculate Statistical Metrics\n","        # =============================================================================\n","        print(f\"\\nSTATISTICAL METRICS FOR {code}\")\n","        print(\"-\" * 50)\n","\n","        train_metrics = calculate_metrics(Y_Train_plt, Y_Train_Pred_plt)\n","        test_metrics = calculate_metrics(Y_Test_plt, Y_Test_Pred_plt)\n","        improvement_metrics = calculate_improvement(train_metrics, test_metrics)\n","\n","        # Print metrics table\n","        print(f\"{'Metric':<10} {'Training':<12} {'Test':<12} {'Improvement':<12}\")\n","        print(\"-\" * 50)\n","        for metric in ['MAE', 'RMSE', 'R2', 'MAPE', 'MPE']:\n","            train_val = train_metrics[metric]\n","            test_val = test_metrics[metric]\n","            improvement_val = improvement_metrics[metric]\n","\n","            if metric in ['MAE', 'RMSE']:\n","                print(f\"{metric:<10} {train_val:<12.4f} {test_val:<12.4f} {improvement_val:<12.4f}\")\n","            elif metric == 'R2':\n","                print(f\"{metric:<10} {train_val:<12.4f} {test_val:<12.4f} {improvement_val:<12.4f}\")\n","            else:  # MAPE, MPE (percentages)\n","                print(f\"{metric:<10} {train_val:<12.2f}% {test_val:<12.2f}% {improvement_val:<12.2f}%\")\n","\n","        # =============================================================================\n","        # Save Metrics to CSV\n","        # =============================================================================\n","        metrics_df = pd.DataFrame({\n","            'Metric': list(train_metrics.keys()),\n","            'Training': list(train_metrics.values()),\n","            'Test': list(test_metrics.values()),\n","            'Improvement': list(improvement_metrics.values())\n","        })\n","        metrics_df.to_csv(f\"{property_outputDir}/{code}_metrics.csv\", index=False)\n","\n","        # =============================================================================\n","        # Generate and Save Plots\n","        # =============================================================================\n","        print(f\"Generating plots for {code}...\")\n","        train_metrics_plot, test_metrics_plot, combined_metrics_plot = plot_predictions(\n","            Y_Train_plt, Y_Train_Pred_plt, Y_Test_plt, Y_Test_Pred_plt,\n","            Y_Train_CI_plt, Y_Test_CI_plt, code, property_outputDir\n","        )\n","\n","        # =============================================================================\n","        # Print Analysis Summary\n","        # =============================================================================\n","        print(f\"\\nConfidence Interval Analysis for {code}:\")\n","        print(f\"Training: {count_CI[0]}/{len(Y_Train_plt)} ({count_CI[1]*100:.1f}%) outside 95% CI\")\n","        print(f\"Test: {count_CI[2]}/{len(Y_Test_plt)} ({count_CI[3]*100:.1f}%) outside 95% CI\")\n","\n","        if remove_outliers_flag:\n","            print(f\"\\nOutlier Removal Summary for {code}:\")\n","            print(f\"Original samples: {outlier_info['n_original']}\")\n","            print(f\"Outliers removed: {outlier_info['n_outliers']} ({outlier_info['outlier_percentage']:.1f}%)\")\n","            print(f\"Final samples used: {outlier_info['n_clean']}\")\n","\n","        # =============================================================================\n","        # Save Outlier Information\n","        # =============================================================================\n","        if outlier_info and outlier_info['n_outliers'] > 0:\n","            # Save outlier details to CSV\n","            outlier_df = pd.DataFrame({\n","                'Original_Index': outlier_info['outlier_indices'],\n","                'Outlier_Value': outlier_info['outlier_values']\n","            })\n","            outlier_df.to_csv(f\"{property_outputDir}/{code}_outliers_removed.csv\", index=False)\n","\n","            # Save outlier removal summary\n","            with open(f\"{property_outputDir}/{code}_outlier_summary.txt\", 'w') as f:\n","                f.write(f\"Outlier Removal Summary for {code}\\n\")\n","                f.write(f\"{'='*40}\\n\")\n","                f.write(f\"Method: {outlier_info['method']}\\n\")\n","                f.write(f\"Threshold: {outlier_info['threshold']}\\n\")\n","                f.write(f\"Original samples: {outlier_info['n_original']}\\n\")\n","                f.write(f\"Outliers removed: {outlier_info['n_outliers']}\\n\")\n","                f.write(f\"Outlier percentage: {outlier_info['outlier_percentage']:.2f}%\\n\")\n","                f.write(f\"Final samples: {outlier_info['n_clean']}\\n\")\n","                if outlier_info['n_outliers'] > 0:\n","                    f.write(f\"Outlier value range: [{np.min(outlier_info['outlier_values']):.2f}, {np.max(outlier_info['outlier_values']):.2f}]\\n\")\n","\n","        # =============================================================================\n","        # Create Comprehensive Results Summary\n","        # =============================================================================\n","        results_summary = {\n","            'Property': code,\n","            'Method': method_number,\n","            'Kernel': kernel,\n","            'N_Original': outlier_info['n_original'],\n","            'N_Outliers_Removed': outlier_info['n_outliers'],\n","            'Outlier_Percentage': outlier_info['outlier_percentage'],\n","            'Outlier_Method': outlier_info['method'],\n","            'Outlier_Threshold': outlier_info['threshold'],\n","            'N_Train': len(Y_Train_plt),\n","            'N_Test': len(Y_Test_plt),\n","            'N_Total': len(Y_Train_plt) + len(Y_Test_plt),\n","            'LML': best_lml,\n","            'Condition_Number': cond_num,\n","            'Fit_Success': fit_success,\n","            'Train_R2': train_metrics['R2'],\n","            'Test_R2': test_metrics['R2'],\n","            'Combined_R2': combined_metrics_plot['R2'],\n","            'Train_MAE': train_metrics['MAE'],\n","            'Test_MAE': test_metrics['MAE'],\n","            'Combined_MAE': combined_metrics_plot['MAE'],\n","            'Train_RMSE': train_metrics['RMSE'],\n","            'Test_RMSE': test_metrics['RMSE'],\n","            'Combined_RMSE': combined_metrics_plot['RMSE'],\n","            'Train_MAPE': train_metrics['MAPE'],\n","            'Test_MAPE': test_metrics['MAPE'],\n","            'Combined_MAPE': combined_metrics_plot['MAPE'],\n","            'Train_MPE': train_metrics['MPE'],\n","            'Test_MPE': test_metrics['MPE'],\n","            'Combined_MPE': combined_metrics_plot['MPE'],\n","            'R2_Improvement': improvement_metrics['R2'],\n","            'MAE_Improvement': improvement_metrics['MAE'],\n","            'RMSE_Improvement': improvement_metrics['RMSE'],\n","            'MAPE_Improvement': improvement_metrics['MAPE'],\n","            'MPE_Improvement': improvement_metrics['MPE'],\n","            'CI_Train_Outside': count_CI[1],\n","            'CI_Test_Outside': count_CI[3]\n","        }\n","\n","        # =============================================================================\n","        # Save Summary to JSON\n","        # =============================================================================\n","        import json\n","        with open(f\"{property_outputDir}/{code}_summary.json\", 'w') as f:\n","            json.dump(results_summary, f, indent=4, default=str)\n","\n","        print(f\"\\nResults saved to: {property_outputDir}\")\n","        print(f\"Files created:\")\n","        print(f\"  - {code}_GP_results.png (side-by-side plots)\")\n","        print(f\"  - {code}_metrics.csv (metrics table with improvement)\")\n","        print(f\"  - {code}_summary.json (complete summary)\")\n","        if outlier_info['n_outliers'] > 0:\n","            print(f\"  - {code}_outliers_removed.csv (outlier details)\")\n","            print(f\"  - {code}_outlier_summary.txt (outlier summary)\")\n","\n","        return results_summary\n","\n","    except Exception as e:\n","        print(f\"Error processing {code}: {str(e)}\")\n","        import traceback\n","        traceback.print_exc()\n","        return None\n","\n","def print_comprehensive_metrics_table(all_results):\n","    \"\"\"Print a comprehensive table of statistical metrics for all properties\"\"\"\n","\n","    if not all_results:\n","        print(\"No results to display.\")\n","        return\n","\n","    print(f\"\\n{'='*120}\")\n","    print(f\"COMPREHENSIVE STATISTICAL METRICS TABLE - ALL PROPERTIES\")\n","    print(f\"{'='*120}\")\n","\n","    # Define metrics to display\n","    metrics = ['R2', 'MAE', 'RMSE', 'MAPE', 'MPE']\n","\n","    # Create header\n","    header = f\"{'Property':<10}\"\n","    for dataset in ['Train', 'Test', 'Combined']:\n","        header += f\"{'':>20}{dataset:<20}{'':>20}\"\n","    print(header)\n","\n","    # Create sub-header with metric names\n","    subheader = f\"{'':>10}\"\n","    for dataset in ['Train', 'Test', 'Combined']:\n","        subheader += f\"{'R²':<8}{'MAE':<8}{'RMSE':<8}{'MAPE':<8}{'MPE':<8}{'':>5}\"\n","    print(subheader)\n","\n","    print(\"-\" * 120)\n","\n","    # Print data for each property\n","    for result in all_results:\n","        property_name = result['Property']\n","        line = f\"{property_name:<10}\"\n","\n","        # Train metrics\n","        line += f\"{result['Train_R2']:<8.3f}\"\n","        line += f\"{result['Train_MAE']:<8.2f}\"\n","        line += f\"{result['Train_RMSE']:<8.2f}\"\n","        line += f\"{result['Train_MAPE']:<8.1f}\"\n","        line += f\"{result['Train_MPE']:<8.1f}\"\n","        line += f\"{'':>5}\"\n","\n","        # Test metrics\n","        line += f\"{result['Test_R2']:<8.3f}\"\n","        line += f\"{result['Test_MAE']:<8.2f}\"\n","        line += f\"{result['Test_RMSE']:<8.2f}\"\n","        line += f\"{result['Test_MAPE']:<8.1f}\"\n","        line += f\"{result['Test_MPE']:<8.1f}\"\n","        line += f\"{'':>5}\"\n","\n","        # Combined metrics\n","        line += f\"{result['Combined_R2']:<8.3f}\"\n","        line += f\"{result['Combined_MAE']:<8.2f}\"\n","        line += f\"{result['Combined_RMSE']:<8.2f}\"\n","        line += f\"{result['Combined_MAPE']:<8.1f}\"\n","        line += f\"{result['Combined_MPE']:<8.1f}\"\n","\n","        print(line)\n","\n","    print(\"-\" * 120)\n","\n","    # Calculate and print summary statistics\n","    print(f\"\\nSUMMARY STATISTICS ACROSS ALL PROPERTIES:\")\n","    print(f\"{'Metric':<15}{'Train':<15}{'Test':<15}{'Combined':<15}\")\n","    print(\"-\" * 60)\n","\n","    for metric in ['R2', 'MAE', 'RMSE', 'MAPE', 'MPE']:\n","        train_key = f'Train_{metric}'\n","        test_key = f'Test_{metric}'\n","        combined_key = f'Combined_{metric}'\n","\n","        train_values = [r[train_key] for r in all_results if not np.isnan(r[train_key]) and not np.isinf(r[train_key])]\n","        test_values = [r[test_key] for r in all_results if not np.isnan(r[test_key]) and not np.isinf(r[test_key])]\n","        combined_values = [r[combined_key] for r in all_results if not np.isnan(r[combined_key]) and not np.isinf(r[combined_key])]\n","\n","        if train_values and test_values and combined_values:\n","            train_mean = np.mean(train_values)\n","            test_mean = np.mean(test_values)\n","            combined_mean = np.mean(combined_values)\n","\n","            print(f\"{metric + ' (avg)':<15}{train_mean:<15.3f}{test_mean:<15.3f}{combined_mean:<15.3f}\")\n","\n","\n","def print_performance_ranking_table(all_results):\n","    \"\"\"Print properties ranked by performance\"\"\"\n","\n","    if not all_results:\n","        return\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"PROPERTY PERFORMANCE RANKING (by Test R²)\")\n","    print(f\"{'='*80}\")\n","\n","    # Sort by Test R2 (descending)\n","    sorted_results = sorted(all_results, key=lambda x: x['Test_R2'], reverse=True)\n","\n","    print(f\"{'Rank':<6}{'Property':<10}{'Test R²':<10}{'Test RMSE':<12}{'Test MAE':<10}{'Overfitting':<12}\")\n","    print(\"-\" * 80)\n","\n","    for i, result in enumerate(sorted_results, 1):\n","        # Calculate overfitting indicator (Train R2 - Test R2)\n","        overfitting = result['Train_R2'] - result['Test_R2']\n","        overfitting_indicator = \"High\" if overfitting > 0.1 else \"Moderate\" if overfitting > 0.05 else \"Low\"\n","\n","        print(f\"{i:<6}{result['Property']:<10}{result['Test_R2']:<10.3f}\"\n","              f\"{result['Test_RMSE']:<12.2f}{result['Test_MAE']:<10.2f}{overfitting_indicator:<12}\")\n","\n","    print(\"-\" * 80)\n","\n","    # Identify best and worst\n","    best = sorted_results[0]\n","    worst = sorted_results[-1]\n","\n","    print(f\"\\nBest performing property: {best['Property']} (R² = {best['Test_R2']:.3f})\")\n","    print(f\"Worst performing property: {worst['Property']} (R² = {worst['Test_R2']:.3f})\")\n","    print(f\"Performance range: {worst['Test_R2']:.3f} to {best['Test_R2']:.3f}\")\n","\n","\n","def save_comprehensive_metrics_to_csv(all_results, output_dir):\n","    \"\"\"Save detailed metrics table to CSV\"\"\"\n","\n","    if not all_results:\n","        return\n","\n","    # Create detailed metrics DataFrame\n","    detailed_data = []\n","\n","    for result in all_results:\n","        # Create one row per property with all metrics\n","        row = {\n","            'Property': result['Property'],\n","            'Method': result['Method'],\n","            'Kernel': result['Kernel'],\n","            'N_Train': result['N_Train'],\n","            'N_Test': result['N_Test'],\n","            'N_Total': result['N_Total'],\n","\n","            # Training metrics\n","            'Train_R2': result['Train_R2'],\n","            'Train_MAE': result['Train_MAE'],\n","            'Train_RMSE': result['Train_RMSE'],\n","            'Train_MAPE': result['Train_MAPE'],\n","            'Train_MPE': result['Train_MPE'],\n","\n","            # Test metrics\n","            'Test_R2': result['Test_R2'],\n","            'Test_MAE': result['Test_MAE'],\n","            'Test_RMSE': result['Test_RMSE'],\n","            'Test_MAPE': result['Test_MAPE'],\n","            'Test_MPE': result['Test_MPE'],\n","\n","            # Combined metrics\n","            'Combined_R2': result['Combined_R2'],\n","            'Combined_MAE': result['Combined_MAE'],\n","            'Combined_RMSE': result['Combined_RMSE'],\n","            'Combined_MAPE': result['Combined_MAPE'],\n","            'Combined_MPE': result['Combined_MPE'],\n","\n","            # Improvement metrics\n","            'R2_Improvement': result['R2_Improvement'],\n","            'MAE_Improvement': result['MAE_Improvement'],\n","            'RMSE_Improvement': result['RMSE_Improvement'],\n","            'MAPE_Improvement': result['MAPE_Improvement'],\n","            'MPE_Improvement': result['MPE_Improvement'],\n","\n","            # Other metrics\n","            'LML': result['LML'],\n","            'Condition_Number': result['Condition_Number'],\n","            'Fit_Success': result['Fit_Success'],\n","            'CI_Train_Outside': result['CI_Train_Outside'],\n","            'CI_Test_Outside': result['CI_Test_Outside']\n","        }\n","        detailed_data.append(row)\n","\n","    # Create DataFrame and save\n","    detailed_df = pd.DataFrame(detailed_data)\n","    detailed_df.to_csv(f\"{output_dir}comprehensive_metrics_table.csv\", index=False)\n","\n","    # Create a simplified summary table\n","    summary_data = []\n","    for result in all_results:\n","        summary_row = {\n","            'Property': result['Property'],\n","            'Train_R2': result['Train_R2'],\n","            'Train_RMSE': result['Train_RMSE'],\n","            'Test_R2': result['Test_R2'],\n","            'Test_RMSE': result['Test_RMSE'],\n","            'Combined_R2': result['Combined_R2'],\n","            'Combined_RMSE': result['Combined_RMSE'],\n","            'Overfitting_R2': result['R2_Improvement'],\n","            'N_Train': result['N_Train'],\n","            'N_Test': result['N_Test']\n","        }\n","        summary_data.append(summary_row)\n","\n","    summary_df = pd.DataFrame(summary_data)\n","    summary_df.to_csv(f\"{output_dir}summary_metrics_table.csv\", index=False)\n","\n","    print(f\"\\nDetailed metrics saved to: {output_dir}comprehensive_metrics_table.csv\")\n","    print(f\"Summary metrics saved to: {output_dir}summary_metrics_table.csv\")"],"metadata":{"id":"_AX8PmL5qL_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tb,Tm,H_vap"],"metadata":{"id":"4A1jVNeeMcjG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSXTGK-XXfPB"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    # =============================================================================\n","    # Configuration - Modify these paths as needed\n","    # =============================================================================\n","\n","    # Set your data directory path here\n","    dataDir = '/content/drive/Shareddrives/GCCP/GCCP/Raw_data_files/'\n","    outputDir = '/content/drive/Shareddrives/GCCP/GCCP/Output_files/'\n","    modelBuildingDataDir = '/content/drive/Shareddrives/GCCP/GCCP/Data_Model_Building/'\n","\n","    # Property codes to process\n","    property_codes = ['Tb', 'Tm', 'Hvap']\n","    #property_codes = ['Tc']\n","\n","    # GP Configuration parameters\n","    kernel = 'RBF'  # Options: RQ, RBF, Matern12, Matern32, Matern52\n","    anisotropic = False\n","    opt_method = 'L-BFGS-B'  # Options: L-BFGS-B, BFGS\n","    useWhiteKernel = True\n","    trainLikelihood = False\n","    retrain_GP = 10\n","    seed = 42\n","\n","    # Outlier removal parameters\n","    remove_outliers_flag = True  # Set to False to disable outlier removal\n","    outlier_method = 'iqr'  # Options: 'iqr', 'zscore', 'modified_zscore'\n","    outlier_threshold = 1.5  # 1.5 for IQR, 3.0 for Z-score, 3.5 for modified Z-score\n","\n","    # Initialize timer and results storage\n","    total_start_time = time.time()\n","    all_results = []\n","    successful_properties = []\n","    failed_properties = []\n","\n","    print(f\"{'='*100}\")\n","    print(f\"STARTING AUTOMATED GP TRAINING FOR ALL PROPERTIES (Without outliers)\")\n","    print(f\"{'='*100}\")\n","    print(f\"Properties to process: {property_codes}\")\n","    print(f\"Method: {method_number}, Kernel: {kernel}, Retrains: {retrain_GP}\")\n","    print(f\"Output directory: {outputDir}\")\n","    print(f\"{'='*100}\")\n","\n","    # Process each property\n","    # Process each property (MODIFIED)\n","    for i, code in enumerate(property_codes):\n","        property_start_time = time.time()\n","\n","        print(f\"\\n{'#'*60}\")\n","        print(f\"PROCESSING {i+1}/{len(property_codes)}: {code}\")\n","        print(f\"{'#'*60}\")\n","\n","        # Process the property with outlier removal\n","        result = process_single_property(\n","            code=code,\n","            kernel=kernel,\n","            anisotropic=anisotropic,\n","            opt_method=opt_method,\n","            useWhiteKernel=useWhiteKernel,\n","            trainLikelihood=trainLikelihood,\n","            retrain_GP=retrain_GP,\n","            method_number=method_number,\n","            outputDir=outputDir,\n","            modelBuildingDataDir=modelBuildingDataDir,\n","            seed=seed,\n","            remove_outliers_flag=remove_outliers_flag,\n","            outlier_method=outlier_method,\n","            outlier_threshold=outlier_threshold\n","        )\n","\n","        property_end_time = time.time()\n","        property_elapsed = property_end_time - property_start_time\n","\n","        if result is not None:\n","            all_results.append(result)\n","            successful_properties.append(code)\n","            print(f\"\\n✓ {code} completed successfully in {property_elapsed:.2f} seconds\")\n","        else:\n","            failed_properties.append(code)\n","            print(f\"\\n✗ {code} failed after {property_elapsed:.2f} seconds\")\n","\n","    # Create comprehensive summary\n","    total_end_time = time.time()\n","    total_elapsed = total_end_time - total_start_time\n","\n","    print(f\"\\n{'='*100}\")\n","    print(f\" COMPLETE\")\n","    print(f\"{'='*100}\")\n","    print(f\"Total time elapsed: {total_elapsed:.2f} seconds ({total_elapsed/60:.1f} minutes)\")\n","    print(f\"Successfully processed: {len(successful_properties)}/{len(property_codes)} properties\")\n","    print(f\"Successful: {successful_properties}\")\n","    if failed_properties:\n","        print(f\"Failed: {failed_properties}\")\n","\n","    # Save individual results summary (existing code)\n","    if all_results:\n","        # Create overall summary DataFrame\n","        summary_df = pd.DataFrame(all_results)\n","        summary_df.to_csv(f\"{outputDir}all_properties_summary.csv\", index=False)\n","\n","        # NEW: Print comprehensive metrics table\n","        print_comprehensive_metrics_table(all_results)\n","\n","        # NEW: Print performance ranking\n","        print_performance_ranking_table(all_results)\n","\n","        # NEW: Save detailed metrics to CSV\n","        save_comprehensive_metrics_to_csv(all_results, outputDir)\n","\n","        # Quick summary statistics (existing code)\n","        print(f\"\\n{'='*60}\")\n","        print(f\"QUICK SUMMARY\")\n","        print(f\"{'='*60}\")\n","\n","        print(f\"{'Property':<8} {'Test R²':<8} {'Test RMSE':<10} {'Test MAE':<8} {'Train/Test R²':<12}\")\n","        print(\"-\" * 60)\n","        for result in all_results:\n","            r2_ratio = result['Train_R2'] / result['Test_R2'] if result['Test_R2'] > 0 else np.inf\n","            print(f\"{result['Property']:<8} {result['Test_R2']:<8.3f} {result['Test_RMSE']:<10.2f} \"\n","                  f\"{result['Test_MAE']:<8.2f} {r2_ratio:<12.2f}\")\n","\n","        # Best and worst performing properties\n","        best_r2 = max(all_results, key=lambda x: x['Test_R2'])\n","        worst_r2 = min(all_results, key=lambda x: x['Test_R2'])\n","\n","        print(f\"\\nBest R² performance: {best_r2['Property']} (R² = {best_r2['Test_R2']:.3f})\")\n","        print(f\"Worst R² performance: {worst_r2['Property']} (R² = {worst_r2['Test_R2']:.3f})\")\n","\n","        print(f\"\\nAll results saved to: {outputDir}all_properties_summary.csv\")\n","\n","    print(f\"\\n{'='*100}\")\n","    print(\" FINISHED\")\n","    print(f\"{'='*100}\")"]},{"cell_type":"markdown","source":["# Pc, Vc, Tc"],"metadata":{"id":"URgNAywnMkJ9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"phqch7Hlm4T9"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    # =============================================================================\n","    # Configuration - Modify these paths as needed\n","    # =============================================================================\n","\n","    # Set your data directory path here\n","    dataDir = '/content/drive/Shareddrives/GCCP/GCCP/Raw_data_files/'\n","    outputDir = '/content/drive/Shareddrives/GCCP/GCCP/Output_files/'\n","    modelBuildingDataDir = '/content/drive/Shareddrives/GCCP/GCCP/Data_Model_Building/'\n","\n","    # Property codes to process\n","    property_codes = ['Pc', 'Vc', 'Tc']\n","    #property_codes = ['Tc']\n","\n","    # GP Configuration parameters\n","    kernel = 'RBF'  # Options: RQ, RBF, Matern12, Matern32, Matern52\n","    anisotropic = False\n","    opt_method = 'L-BFGS-B'  # Options: L-BFGS-B, BFGS\n","    useWhiteKernel = True\n","    trainLikelihood = False\n","    retrain_GP = 10\n","    seed = 42\n","\n","    # Outlier removal parameters\n","    remove_outliers_flag = True  # Set to False to disable outlier removal\n","    outlier_method = 'iqr'  # Options: 'iqr', 'zscore', 'modified_zscore'\n","    outlier_threshold = 1.5  # 1.5 for IQR, 3.0 for Z-score, 3.5 for modified Z-score\n","\n","    # Initialize timer and results storage\n","    total_start_time = time.time()\n","    all_results = []\n","    successful_properties = []\n","    failed_properties = []\n","\n","    print(f\"{'='*100}\")\n","    print(f\"STARTING AUTOMATED GP TRAINING FOR ALL PROPERTIES (Without outliers)\")\n","    print(f\"{'='*100}\")\n","    print(f\"Properties to process: {property_codes}\")\n","    print(f\"Method: {method_number}, Kernel: {kernel}, Retrains: {retrain_GP}\")\n","    print(f\"Output directory: {outputDir}\")\n","    print(f\"{'='*100}\")\n","\n","    # Process each property\n","    # Process each property (MODIFIED)\n","    for i, code in enumerate(property_codes):\n","        property_start_time = time.time()\n","\n","        print(f\"\\n{'#'*60}\")\n","        print(f\"PROCESSING {i+1}/{len(property_codes)}: {code}\")\n","        print(f\"{'#'*60}\")\n","\n","        # Process the property with outlier removal\n","        result = process_single_property(\n","            code=code,\n","            kernel=kernel,\n","            anisotropic=anisotropic,\n","            opt_method=opt_method,\n","            useWhiteKernel=useWhiteKernel,\n","            trainLikelihood=trainLikelihood,\n","            retrain_GP=retrain_GP,\n","            method_number=method_number,\n","            outputDir=outputDir,\n","            modelBuildingDataDir=modelBuildingDataDir,\n","            seed=seed,\n","            remove_outliers_flag=remove_outliers_flag,\n","            outlier_method=outlier_method,\n","            outlier_threshold=outlier_threshold\n","        )\n","\n","        property_end_time = time.time()\n","        property_elapsed = property_end_time - property_start_time\n","\n","        if result is not None:\n","            all_results.append(result)\n","            successful_properties.append(code)\n","            print(f\"\\n✓ {code} completed successfully in {property_elapsed:.2f} seconds\")\n","        else:\n","            failed_properties.append(code)\n","            print(f\"\\n✗ {code} failed after {property_elapsed:.2f} seconds\")\n","\n","    # Create comprehensive summary\n","    total_end_time = time.time()\n","    total_elapsed = total_end_time - total_start_time\n","\n","    print(f\"\\n{'='*100}\")\n","    print(f\" COMPLETE\")\n","    print(f\"{'='*100}\")\n","    print(f\"Total time elapsed: {total_elapsed:.2f} seconds ({total_elapsed/60:.1f} minutes)\")\n","    print(f\"Successfully processed: {len(successful_properties)}/{len(property_codes)} properties\")\n","    print(f\"Successful: {successful_properties}\")\n","    if failed_properties:\n","        print(f\"Failed: {failed_properties}\")\n","\n","    # Save individual results summary (existing code)\n","    if all_results:\n","        # Create overall summary DataFrame\n","        summary_df = pd.DataFrame(all_results)\n","        summary_df.to_csv(f\"{outputDir}all_properties_summary.csv\", index=False)\n","\n","        # NEW: Print comprehensive metrics table\n","        print_comprehensive_metrics_table(all_results)\n","\n","        # NEW: Print performance ranking\n","        print_performance_ranking_table(all_results)\n","\n","        # NEW: Save detailed metrics to CSV\n","        save_comprehensive_metrics_to_csv(all_results, outputDir)\n","\n","        # Quick summary statistics (existing code)\n","        print(f\"\\n{'='*60}\")\n","        print(f\"QUICK SUMMARY\")\n","        print(f\"{'='*60}\")\n","\n","        print(f\"{'Property':<8} {'Test R²':<8} {'Test RMSE':<10} {'Test MAE':<8} {'Train/Test R²':<12}\")\n","        print(\"-\" * 60)\n","        for result in all_results:\n","            r2_ratio = result['Train_R2'] / result['Test_R2'] if result['Test_R2'] > 0 else np.inf\n","            print(f\"{result['Property']:<8} {result['Test_R2']:<8.3f} {result['Test_RMSE']:<10.2f} \"\n","                  f\"{result['Test_MAE']:<8.2f} {r2_ratio:<12.2f}\")\n","\n","        # Best and worst performing properties\n","        best_r2 = max(all_results, key=lambda x: x['Test_R2'])\n","        worst_r2 = min(all_results, key=lambda x: x['Test_R2'])\n","\n","        print(f\"\\nBest R² performance: {best_r2['Property']} (R² = {best_r2['Test_R2']:.3f})\")\n","        print(f\"Worst R² performance: {worst_r2['Property']} (R² = {worst_r2['Test_R2']:.3f})\")\n","\n","        print(f\"\\nAll results saved to: {outputDir}all_properties_summary.csv\")\n","\n","    print(f\"\\n{'='*100}\")\n","    print(\" FINISHED\")\n","    print(f\"{'='*100}\")"]},{"cell_type":"markdown","source":["# logP"],"metadata":{"id":"8FWVpSuzm9hE"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # =============================================================================\n","    # Configuration - Modify these paths as needed\n","    # =============================================================================\n","\n","    # Set your data directory path here\n","    dataDir = '/content/drive/Shareddrives/GCCP/GCCP/Raw_data_files/'\n","    outputDir = '/content/drive/Shareddrives/GCCP/GCCP/Output_files/'\n","    modelBuildingDataDir = '/content/drive/Shareddrives/GCCP/GCCP/Data_Model_Building/'\n","\n","    # Property codes to process\n","    property_codes = ['logP']\n","    #property_codes = ['Tc']\n","\n","    # GP Configuration parameters\n","    kernel = 'RBF'  # Options: RQ, RBF, Matern12, Matern32, Matern52\n","    anisotropic = False\n","    opt_method = 'L-BFGS-B'  # Options: L-BFGS-B, BFGS\n","    useWhiteKernel = True\n","    trainLikelihood = False\n","    retrain_GP = 10\n","    seed = 42\n","\n","    # Outlier removal parameters\n","    remove_outliers_flag = True  # Set to False to disable outlier removal\n","    outlier_method = 'iqr'  # Options: 'iqr', 'zscore', 'modified_zscore'\n","    outlier_threshold = 1.5  # 1.5 for IQR, 3.0 for Z-score, 3.5 for modified Z-score\n","\n","    # Initialize timer and results storage\n","    total_start_time = time.time()\n","    all_results = []\n","    successful_properties = []\n","    failed_properties = []\n","\n","    print(f\"{'='*100}\")\n","    print(f\"STARTING AUTOMATED GP TRAINING FOR ALL PROPERTIES (Without outliers)\")\n","    print(f\"{'='*100}\")\n","    print(f\"Properties to process: {property_codes}\")\n","    print(f\"Method: {method_number}, Kernel: {kernel}, Retrains: {retrain_GP}\")\n","    print(f\"Output directory: {outputDir}\")\n","    print(f\"{'='*100}\")\n","\n","    # Process each property\n","    # Process each property (MODIFIED)\n","    for i, code in enumerate(property_codes):\n","        property_start_time = time.time()\n","\n","        print(f\"\\n{'#'*60}\")\n","        print(f\"PROCESSING {i+1}/{len(property_codes)}: {code}\")\n","        print(f\"{'#'*60}\")\n","\n","        # Process the property with outlier removal\n","        result = process_single_property(\n","            code=code,\n","            kernel=kernel,\n","            anisotropic=anisotropic,\n","            opt_method=opt_method,\n","            useWhiteKernel=useWhiteKernel,\n","            trainLikelihood=trainLikelihood,\n","            retrain_GP=retrain_GP,\n","            method_number=method_number,\n","            outputDir=outputDir,\n","            modelBuildingDataDir=modelBuildingDataDir,\n","            seed=seed,\n","            remove_outliers_flag=remove_outliers_flag,\n","            outlier_method=outlier_method,\n","            outlier_threshold=outlier_threshold\n","        )\n","\n","        property_end_time = time.time()\n","        property_elapsed = property_end_time - property_start_time\n","\n","        if result is not None:\n","            all_results.append(result)\n","            successful_properties.append(code)\n","            print(f\"\\n✓ {code} completed successfully in {property_elapsed:.2f} seconds\")\n","        else:\n","            failed_properties.append(code)\n","            print(f\"\\n✗ {code} failed after {property_elapsed:.2f} seconds\")\n","\n","    # Create comprehensive summary\n","    total_end_time = time.time()\n","    total_elapsed = total_end_time - total_start_time\n","\n","    print(f\"\\n{'='*100}\")\n","    print(f\" COMPLETE\")\n","    print(f\"{'='*100}\")\n","    print(f\"Total time elapsed: {total_elapsed:.2f} seconds ({total_elapsed/60:.1f} minutes)\")\n","    print(f\"Successfully processed: {len(successful_properties)}/{len(property_codes)} properties\")\n","    print(f\"Successful: {successful_properties}\")\n","    if failed_properties:\n","        print(f\"Failed: {failed_properties}\")\n","\n","    # Save individual results summary (existing code)\n","    if all_results:\n","        # Create overall summary DataFrame\n","        summary_df = pd.DataFrame(all_results)\n","        summary_df.to_csv(f\"{outputDir}all_properties_summary.csv\", index=False)\n","\n","        # NEW: Print comprehensive metrics table\n","        print_comprehensive_metrics_table(all_results)\n","\n","        # NEW: Print performance ranking\n","        print_performance_ranking_table(all_results)\n","\n","        # NEW: Save detailed metrics to CSV\n","        save_comprehensive_metrics_to_csv(all_results, outputDir)\n","\n","        # Quick summary statistics (existing code)\n","        print(f\"\\n{'='*60}\")\n","        print(f\"QUICK SUMMARY\")\n","        print(f\"{'='*60}\")\n","\n","        print(f\"{'Property':<8} {'Test R²':<8} {'Test RMSE':<10} {'Test MAE':<8} {'Train/Test R²':<12}\")\n","        print(\"-\" * 60)\n","        for result in all_results:\n","            r2_ratio = result['Train_R2'] / result['Test_R2'] if result['Test_R2'] > 0 else np.inf\n","            print(f\"{result['Property']:<8} {result['Test_R2']:<8.3f} {result['Test_RMSE']:<10.2f} \"\n","                  f\"{result['Test_MAE']:<8.2f} {r2_ratio:<12.2f}\")\n","\n","        # Best and worst performing properties\n","        best_r2 = max(all_results, key=lambda x: x['Test_R2'])\n","        worst_r2 = min(all_results, key=lambda x: x['Test_R2'])\n","\n","        print(f\"\\nBest R² performance: {best_r2['Property']} (R² = {best_r2['Test_R2']:.3f})\")\n","        print(f\"Worst R² performance: {worst_r2['Property']} (R² = {worst_r2['Test_R2']:.3f})\")\n","\n","        print(f\"\\nAll results saved to: {outputDir}all_properties_summary.csv\")\n","\n","    print(f\"\\n{'='*100}\")\n","    print(\" FINISHED\")\n","    print(f\"{'='*100}\")"],"metadata":{"id":"PMQQGavSMvgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2NzUILZkM1g4"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}